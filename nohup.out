INFO 12-11 07:21:00 api_server.py:585] vLLM API server version 0.6.4.post1
INFO 12-11 07:21:00 api_server.py:586] args: Namespace(subparser='serve', model_tag='/scratch/mc_lmy/models/JARVIS/checkpoints/mc_llava_v1.6_vicuna_7b-full-11-10-craft-craft_table-shell_agent-hard-llama-2-h0-c1-b512-12-08-1-A100-c4-e3-b16-a4/checkpoint-2000', config='', host=None, port=9213, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template='/home/limuyao/workspace/jarvis-train/ultron/model/inference/template/template_llava.jinja', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/scratch/mc_lmy/models/JARVIS/checkpoints/mc_llava_v1.6_vicuna_7b-full-11-10-craft-craft_table-shell_agent-hard-llama-2-h0-c1-b512-12-08-1-A100-c4-e3-b16-a4/checkpoint-2000', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', chat_template_text_format='string', trust_remote_code=True, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=3072, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=2, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.95, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7f16c4b5f0a0>)
INFO 12-11 07:21:00 api_server.py:175] Multiprocessing frontend to use ipc:///tmp/a91ae761-1c61-4f52-b57c-c7fabeafc406 for IPC Path.
INFO 12-11 07:21:00 api_server.py:194] Started engine process with PID 175944
INFO 12-11 07:21:06 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.
INFO 12-11 07:21:06 config.py:1020] Defaulting to use mp for distributed inference
WARNING 12-11 07:21:06 arg_utils.py:1075] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
INFO 12-11 07:21:10 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.
INFO 12-11 07:21:10 config.py:1020] Defaulting to use mp for distributed inference
WARNING 12-11 07:21:10 arg_utils.py:1075] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. block manager v2), please file an issue with detailed information.
INFO 12-11 07:21:10 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='/scratch/mc_lmy/models/JARVIS/checkpoints/mc_llava_v1.6_vicuna_7b-full-11-10-craft-craft_table-shell_agent-hard-llama-2-h0-c1-b512-12-08-1-A100-c4-e3-b16-a4/checkpoint-2000', speculative_config=None, tokenizer='/scratch/mc_lmy/models/JARVIS/checkpoints/mc_llava_v1.6_vicuna_7b-full-11-10-craft-craft_table-shell_agent-hard-llama-2-h0-c1-b512-12-08-1-A100-c4-e3-b16-a4/checkpoint-2000', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=3072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/scratch/mc_lmy/models/JARVIS/checkpoints/mc_llava_v1.6_vicuna_7b-full-11-10-craft-craft_table-shell_agent-hard-llama-2-h0-c1-b512-12-08-1-A100-c4-e3-b16-a4/checkpoint-2000, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=True, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)
WARNING 12-11 07:21:10 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 12-11 07:21:10 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
INFO 12-11 07:21:10 selector.py:135] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=176392)[0;0m INFO 12-11 07:21:14 selector.py:135] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=176392)[0;0m INFO 12-11 07:21:14 multiproc_worker_utils.py:215] Worker ready; awaiting tasks
INFO 12-11 07:21:15 utils.py:961] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=176392)[0;0m INFO 12-11 07:21:15 utils.py:961] Found nccl from library libnccl.so.2
INFO 12-11 07:21:15 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=176392)[0;0m INFO 12-11 07:21:15 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=176392)[0;0m INFO 12-11 07:21:15 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/limuyao/.cache/vllm/gpu_p2p_access_cache_for_5,6.json
INFO 12-11 07:21:15 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/limuyao/.cache/vllm/gpu_p2p_access_cache_for_5,6.json
INFO 12-11 07:21:15 shm_broadcast.py:236] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f52a3295bd0>, local_subscribe_port=44113, remote_subscribe_port=None)
INFO 12-11 07:21:15 model_runner.py:1072] Starting to load model /scratch/mc_lmy/models/JARVIS/checkpoints/mc_llava_v1.6_vicuna_7b-full-11-10-craft-craft_table-shell_agent-hard-llama-2-h0-c1-b512-12-08-1-A100-c4-e3-b16-a4/checkpoint-2000...
[1;36m(VllmWorkerProcess pid=176392)[0;0m INFO 12-11 07:21:15 model_runner.py:1072] Starting to load model /scratch/mc_lmy/models/JARVIS/checkpoints/mc_llava_v1.6_vicuna_7b-full-11-10-craft-craft_table-shell_agent-hard-llama-2-h0-c1-b512-12-08-1-A100-c4-e3-b16-a4/checkpoint-2000...
/home/limuyao/miniconda3/envs/mark3/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
[1;36m(VllmWorkerProcess pid=176392)[0;0m /home/limuyao/miniconda3/envs/mark3/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
[1;36m(VllmWorkerProcess pid=176392)[0;0m   warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]

Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:01<00:03,  1.60s/it]

Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:03<00:01,  1.79s/it]

Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:05<00:00,  1.89s/it]

Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:05<00:00,  1.84s/it]

INFO 12-11 07:21:22 model_runner.py:1077] Loading model weights took 6.6252 GB
[1;36m(VllmWorkerProcess pid=176392)[0;0m INFO 12-11 07:21:22 model_runner.py:1077] Loading model weights took 6.6252 GB
[1;36m(VllmWorkerProcess pid=176392)[0;0m INFO 12-11 07:21:23 worker.py:232] Memory profiling results: total_gpu_memory=79.14GiB initial_memory_usage=7.83GiB peak_torch_memory=6.92GiB memory_usage_post_profile=8.63GiB non_torch_memory=1.99GiB kv_cache_size=66.27GiB gpu_memory_utilization=0.95
INFO 12-11 07:21:23 worker.py:232] Memory profiling results: total_gpu_memory=79.14GiB initial_memory_usage=7.83GiB peak_torch_memory=6.92GiB memory_usage_post_profile=8.63GiB non_torch_memory=1.99GiB kv_cache_size=66.27GiB gpu_memory_utilization=0.95
INFO 12-11 07:21:23 distributed_gpu_executor.py:57] # GPU blocks: 16964, # CPU blocks: 1024
INFO 12-11 07:21:23 distributed_gpu_executor.py:61] Maximum concurrency for 3072 tokens per request: 88.35x
INFO 12-11 07:21:25 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 12-11 07:21:25 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=176392)[0;0m INFO 12-11 07:21:25 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=176392)[0;0m INFO 12-11 07:21:25 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 12-11 07:21:35 custom_all_reduce.py:224] Registering 2275 cuda graph addresses
[1;36m(VllmWorkerProcess pid=176392)[0;0m INFO 12-11 07:21:35 custom_all_reduce.py:224] Registering 2275 cuda graph addresses
[1;36m(VllmWorkerProcess pid=176392)[0;0m INFO 12-11 07:21:35 model_runner.py:1518] Graph capturing finished in 10 secs, took 0.21 GiB
INFO 12-11 07:21:35 model_runner.py:1518] Graph capturing finished in 10 secs, took 0.21 GiB
INFO 12-11 07:21:36 api_server.py:249] vLLM to use /tmp/tmpnvxu7q3g as PROMETHEUS_MULTIPROC_DIR
INFO 12-11 07:21:36 chat_utils.py:431] Using supplied chat template:
INFO 12-11 07:21:36 chat_utils.py:431] {% for message in messages %}
INFO 12-11 07:21:36 chat_utils.py:431]     {% if message['role'] != 'system' %}
INFO 12-11 07:21:36 chat_utils.py:431]         {{ " "+message['role'].upper() + ': '}}
INFO 12-11 07:21:36 chat_utils.py:431]     {% endif %}
INFO 12-11 07:21:36 chat_utils.py:431]     {# Render all images first #}{% for content in message['content'] | selectattr('type', 'equalto', 'image') %}{{ '<image>\n' }}{% endfor %}{# Render all text next #}
INFO 12-11 07:21:36 chat_utils.py:431]     {% if message['role'] != 'assistant' %}
INFO 12-11 07:21:36 chat_utils.py:431]         {% for content in message['content'] | selectattr('type', 'equalto', 'text') %}
INFO 12-11 07:21:36 chat_utils.py:431]             {{ content['text'] + ' '}}
INFO 12-11 07:21:36 chat_utils.py:431]         {% endfor %}{% else %}
INFO 12-11 07:21:36 chat_utils.py:431]         {% for content in message['content'] | selectattr('type', 'equalto', 'text') %}
INFO 12-11 07:21:36 chat_utils.py:431]             {% generation %}
INFO 12-11 07:21:36 chat_utils.py:431]             {{ content['text'] + ' '}}
INFO 12-11 07:21:36 chat_utils.py:431]             {% endgeneration %}
INFO 12-11 07:21:36 chat_utils.py:431]         {% endfor %}
INFO 12-11 07:21:36 chat_utils.py:431]     {% endif %}
INFO 12-11 07:21:36 chat_utils.py:431] {% endfor %}
INFO 12-11 07:21:36 chat_utils.py:431] {% if add_generation_prompt %}
INFO 12-11 07:21:36 chat_utils.py:431]     {{ 'ASSISTANT:' }}
INFO 12-11 07:21:36 chat_utils.py:431] {% endif %}
INFO 12-11 07:21:36 chat_utils.py:431] Using supplied chat template:
INFO 12-11 07:21:36 chat_utils.py:431] {% for message in messages %}
INFO 12-11 07:21:36 chat_utils.py:431]     {% if message['role'] != 'system' %}
INFO 12-11 07:21:36 chat_utils.py:431]         {{ " "+message['role'].upper() + ': '}}
INFO 12-11 07:21:36 chat_utils.py:431]     {% endif %}
INFO 12-11 07:21:36 chat_utils.py:431]     {# Render all images first #}{% for content in message['content'] | selectattr('type', 'equalto', 'image') %}{{ '<image>\n' }}{% endfor %}{# Render all text next #}
INFO 12-11 07:21:36 chat_utils.py:431]     {% if message['role'] != 'assistant' %}
INFO 12-11 07:21:36 chat_utils.py:431]         {% for content in message['content'] | selectattr('type', 'equalto', 'text') %}
INFO 12-11 07:21:36 chat_utils.py:431]             {{ content['text'] + ' '}}
INFO 12-11 07:21:36 chat_utils.py:431]         {% endfor %}{% else %}
INFO 12-11 07:21:36 chat_utils.py:431]         {% for content in message['content'] | selectattr('type', 'equalto', 'text') %}
INFO 12-11 07:21:36 chat_utils.py:431]             {% generation %}
INFO 12-11 07:21:36 chat_utils.py:431]             {{ content['text'] + ' '}}
INFO 12-11 07:21:36 chat_utils.py:431]             {% endgeneration %}
INFO 12-11 07:21:36 chat_utils.py:431]         {% endfor %}
INFO 12-11 07:21:36 chat_utils.py:431]     {% endif %}
INFO 12-11 07:21:36 chat_utils.py:431] {% endfor %}
INFO 12-11 07:21:36 chat_utils.py:431] {% if add_generation_prompt %}
INFO 12-11 07:21:36 chat_utils.py:431]     {{ 'ASSISTANT:' }}
INFO 12-11 07:21:36 chat_utils.py:431] {% endif %}
INFO 12-11 07:21:36 launcher.py:19] Available routes are:
INFO 12-11 07:21:36 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET
INFO 12-11 07:21:36 launcher.py:27] Route: /docs, Methods: HEAD, GET
INFO 12-11 07:21:36 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 12-11 07:21:36 launcher.py:27] Route: /redoc, Methods: HEAD, GET
INFO 12-11 07:21:36 launcher.py:27] Route: /health, Methods: GET
INFO 12-11 07:21:36 launcher.py:27] Route: /tokenize, Methods: POST
INFO 12-11 07:21:36 launcher.py:27] Route: /detokenize, Methods: POST
INFO 12-11 07:21:36 launcher.py:27] Route: /v1/models, Methods: GET
INFO 12-11 07:21:36 launcher.py:27] Route: /version, Methods: GET
INFO 12-11 07:21:36 launcher.py:27] Route: /v1/chat/completions, Methods: POST
INFO 12-11 07:21:36 launcher.py:27] Route: /v1/completions, Methods: POST
INFO 12-11 07:21:36 launcher.py:27] Route: /v1/embeddings, Methods: POST
INFO:     Started server process [175833]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:9213 (Press CTRL+C to quit)
INFO 12-11 07:21:46 metrics.py:449] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 12-11 07:21:54 multiproc_worker_utils.p/home/limuyaonating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=176392)[0;0m INFO 12-11 07:21:54 multiproc_worker_utils.py:240] Worker exiting
INFO 12-11 07:21:54 launcher.py:57] Shutting/home/limuyaoI HTTP server.
ERROR 12-11 07:21:54 engine.py:366] MQLLMEngine terminated
ERROR 12-11 07:21:54 engine.py:366] Tracebac/home/limuyaont call last):
ERROR 12-11 07:21:54 engine.py:366]   File "/home/mc_lmy/miniconda3/envs/mark3/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py", line 363, in run_mp_engine
ERROR 12-11 07:21:54 engine.py:366]     engine.start()
ERROR 12-11 07:21:54 engine.py:366]   File "/home/mc_lmy/miniconda3/envs/mark3/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py", line 140, in start
ERROR 12-11 07:21:54 engine.py:366]     self.cleanup()
ERROR 12/home/limuyao engine.py:366]   File "/home/mc_lmy/miniconda3/envs/mark3/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py", line 351, in signal_handler
ERROR 12-11 07:21:54 engine.py:366]     raise KeyboardInterrupt("MQLLMEngine terminated")
ERROR 12/home/limuyao engine.py:366] KeyboardInterrupt: MQLLMEngine terminated
Process SpawnProcess-1:
Tracebac/home/limuyaont call last):
  File "/home/mc_lmy/miniconda3/envs/mark3/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self/home/limuyao
  File "/home/mc_lmy/miniconda3/envs/mark3/lib/python3.10/multiprocessing/process.py", line 108, in run
    self/home/limuyaolf._args, **self._kwargs)
  File "/home/mc_lmy/miniconda3/envs/mark3/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py", line 368, in run_mp_engine
    rais/home/limuyao
  File "/home/mc_lmy/miniconda3/envs/mark3/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py", line 363, in run_mp_engine
    engine.start()
  File "/home/mc_lmy/miniconda3/envs/mark3/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py", line 140, in start
    self.cleanup()
  File "/home/mc_lmy/miniconda3/envs/mark3/lib/python3.10/site-packages/vllm/engine/multiprocessing/engine.py", line 351, in signal_handler
    raise KeyboardInterrupt("MQLLMEngine terminated")
/home/limuyaorrupt: MQLLMEngine terminated
[rank0]:[W1211 07:21:55.992659242 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
/home/mc_lmy/miniconda3/envs/mark3/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
