import re
import numpy as np
from collections import OrderedDict
from typing import Union,List,Dict
import torch
import copy
import pickle
import json
import numpy as np
from tqdm import tqdm
from pathlib import Path
from rich import console
from jarvis.arm.utils.vpt_lib.actions import  ActionTransformer,Buttons
from jarvis.arm.utils.vpt_lib.action_mapping import CameraHierarchicalMapping
from jarvis.arm.utils.vpt_lib.action_translator import CameraQuantizer




def get_special_token(model_id:str = '/nfs-shared/models/llama-3', bases:list = [10,3,3,3,2,2,2,2,2,11,11]) -> list:  #ÂÅáËÆæÊ∞∏Ëøú‰∏ç‰ºöÂá∫Áé∞8641Ëøô‰∏™Êï∞
    '''
    bases: button+camera
    :output: list, ËøîÂõû‰∏Ä‰∏™ÂåÖÂê´ÊâÄÊúâÊú™Áü•tokenÁöÑÂàóË°®
    Function: ÁîüÊàê‰∏Ä‰∏™ÂåÖÂê´ÊâÄÊúâÊú™Áü•tokenÁöÑÂàóË°®, Áî®‰∫éÊ†áËÆ∞Êú™Áü•ÁöÑtoken
    Examples:
    '''
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    token_num = sum(bases)+30
    special_tokens = sorted(list(tokenizer.vocab.items()), key=lambda x: x[-1])[-token_num:]
    return special_tokens

def prepare_for_remap_control_token(tokenizer_type:str = "llama-2",bases:list = [10,3,3,3,2,2,2,2,2,21,21],not_text=True):
    
    tokens = {}
    for i,base in enumerate(bases):
        for j in range(base):
            token = map_control_token(j,i,tokenizer_type,not_text=not_text)
            tokens[token]=(i,j)
    return tokens

def map_control_token(num:int, place:int, tokenizer_type:str = "llama-2",not_text=False) -> str:
    if tokenizer_type == "llama-2":
        special_tokens = [
            (('ÏßÑ', 31536),('‡¶ú', 31537),('Ï≤ú', 31563),('ÎÖÑ', 31571),('ÏÑ∏', 31578),('ÎØº', 31582),('‡µº', 31585),('·º°', 31598),('Ìò∏', 31603),('‡®∞', 31604),
            ("Îèô", 31000), ("Œ•", 31001), ("‚îå", 31002), ("„Éú", 31003), ("ÂÆÆ", 31004), ("„Äè", 31005), ("‡¶Æ", 31006), ("„Äé", 31007), ("ƒº", 31008), ("‡§∂", 31009), ("‡∏õ", 31010), ("‘±", 31011), ("‡§¨", 31012), ("Ïûê", 31013), ("Êîø", 31014), ("‡Ææ", 31015), ("Èó¥", 31016), ("Ô¨Å", 31017), ("Êùæ", 31018), ("·πÉ", 31019), ("Âßã", 31020), ("ÊÅØ", 31021), ("Â∞ë", 31022), ("Êïô", 31023), ("Ëé∑", 31024), ("Âàó", 31025), ("ÂºÄ", 31026), ("·É¢", 31027), ("„ÉØ", 31028), ("·Éô", 31029), ("Áßë", 31030), ("Êò•", 31031), ("Ê≤ª", 31032), ("Âêâ", 31033), ("‡Ω¶", 31034), ("‡∏®", 31035), ("…í", 31036), ("Âè∞", 31037), ("„Éç", 31038), ("·Ä∏", 31039), ("ƒ©", 31040), ("Â∑•", 31041), ("·Ω±", 31042), ("Áü•", 31043),
            ("ÂÖ´", 31044), ("Â†¥", 31045), ("Áîª", 31046), ("Áôæ", 31047), ("‚òÜ", 31048), ("Ë®ò", 31049), 
            ("Âæó", 31050), ("„ÇΩ", 31051), ("Ê∞è", 31052), ("·Ä¨", 31053), ("Ïóê", 31054), ("‡¶≤", 31055), ("·πõ", 31056), ("ÂÖ≥", 31057), ("ƒ°", 31058), 
            ("·Ω≥", 31059), ("‚àë", 31060), ("„Éô", 31061), ("Ê†á", 31062), ("Îãà", 31063), ("·Ω¥", 31064), ("÷µ", 31065), ("Â§ñ", 31066), ("‚ô†", 31067), ("„Çè", 31068), ("Èñì", 31069), ("‡∏†", 31070), ("Ê†°", 31071), ("Âà∂", 31072), ("‡πÅ", 31073), ("Âäõ", 31074), ("ÈñÄ", 31075), ("Â•Ω", 31076), ("“ì", 31077), ("√ô", 31078), ("‚Ñì", 31079), ("÷∂", 31080), ("Îäî", 31081), ("‚îê", 31082), ("‚àó", 31083), ("Êåá", 31084), ("Ëâ≤", 31085), ("Ëøî", 31086), ("È¶¨", 31087), ("ËØ∑", 31088), ("‚â´", 31089), ("È¢®", 31090), ("·Ωπ", 31091), ("Êé•", 31092), ("ÏÑú", 31093), ("‚Ü≥", 31094), ("„Åõ", 31095), ("Âøó", 31096), ("Ã≤", 31097), ("È≠î", 31098), ("“£", 31099), ("Êõ¥", 31100), ("Á®ã", 31101), ("ÍπÄ", 31102), ("ÈÉ°", 31103), 
            ("‡Ωº", 31104), ("≈©", 31105), ("‡¥ö", 31106), ("Âà©", 31107), ("Áúå", 31108), ("Âë®", 31109), ("„Åù", 31110), ("„ÇÑ", 31111), ("Ë∞∑", 31112), ("È¶ô", 31113), ("‚ôØ", 31114), ("„Åò", 31115), ("ÿå", 31116), ("Êúü", 31117), ("‚àÖ", 31118), ("‚îò", 31119), ("Âàù", 31120), ("Á¶è", 31121), ("Áâá", 31122), ("„Ç∂", 31123), ("Âãï", 31124), ("ÂèÇ", 31125), ("ÏÑ±", 31126), ("∆è", 31127), ("‚ï¶", 31128), ("Ïñ¥", 31129), ("·ÉÆ", 31130), ("Áæ©", 31131), ("‡§ö", 31132), ("Ë±°", 31133), ("Âäü", 31134), ("‚ôÇ", 31135), ("ÎèÑ", 31136), ("Í≥†", 31137), ("Ëøá", 31138), ("’æ", 31139), ("Áöá", 31140), ("Áâπ", 31141), ("·∫≠", 31142), ("Èïø", 31143), ("Ëã±", 31144), ("·∫•", 31145), ("‡¥£", 31146), ("–™", 31147), ("‡¶∏", 31148),
            ("ÂÖ∂", 31149), ("‡¶§", 31150), ("ÊµÅ", 31151), ("Èô§", 31152), ("Ïùº", 31153), ("‡ßÅ", 31154), ("·üí", 31155), ("Ê∞∏", 31156), ("Áõ¥", 31157), ("ÏÉÅ", 31158), ("ÂçÉ", 31159), ("·∫Ø", 31160), ("È§®", 31161), ("≈§", 31162), ("Êúù", 31163), ("‡Æü", 31164), ("…£", 31165), ("Âçï", 31166), (" Ä", 31167), ("Ê†º", 31168), ("Âæ∑", 31169), ("Ï†Ñ", 31170), ("‚ò∫", 31171), ("„Éî", 31172), ("Ê≠å", 31173), ("Ëøõ", 31174), ("Èôê", 31175), ("Â§´", 31176), ("Ìä∏", 31177), ("‚ä¢", 31178), ("Âúí", 31179), ("Èáè", 31180), ("Âúü", 31181), ("Êîæ", 31182), ("Á†Å", 31183), ("Á≠â", 31184), ("Á≥ª", 31185), ("‚àº", 31186), ("ËèØ", 31187), ("‚Üµ", 31188), ("ÏÜå", 31189), ("Â∏∏", 31190), ("Âê¶", 31191), ("Ë¶ã", 31192), ("Ê∫ê", 31193), 
            ("ÂÆû", 31195), ("Âçö", 31196), ("Îùº", 31197), ("Ïõê", 31198), ("Î≥¥", 31199), ("‚äï", 31200), ("Ëß£", 31201), ("„Äú", 31202), ("Áî∑", 31203), ("‡¶¶", 31204), ("„Éù", 31205), ("„Çç", 31206), ("ÎÇò", 31207), ("‡ΩÇ", 31208), ("ÁÑ°", 31209), ("√õ", 31210), ("Ã•", 31211), ("“±", 31212), ("Êü•", 31213), ("Ã£", 31214), ("‚ïó", 31215), ("‚ï©", 31216), ("Êù°", 31217), ("‡¶Ø", 31218), ("·ΩÅ", 31219), ("Âæå", 31220), ("‰ªñ", 31221), ("ÁΩë", 31222), ("‡Æ≤", 31223), ("‚âÉ", 31224), ("Ìôî", 31225), ("€ï", 31226), ("Èòø", 31227), ("·Ä±", 31228), ("Êà∑", 31229), ("‚à´", 31230), ("Íµ¨", 31231), ("‡Ω¢", 31232), ("·Äô", 31233), ("‚ñ∏", 31234), ("’¨", 31235), ("‚óã", 31236), ("ÂëΩ", 31237), ("Â∞±", 31238), ("Èæç", 31239), ("Âêõ", 31240), 
            ("Â§è", 31241), ("Ë®Ä", 31243), ("ÂÖà", 31244), ("‚ûú", 31245), ("·É®", 31246), ("·É´", 31247), ("‡®æ", 31248), ("‡Æµ", 31249), ("„Å©", 31250), ("„Éí", 31251), ("‡πÑ", 31252), ("‡Æ©", 31253), ("„Å∞", 31254), ("„ÇÆ", 31255), ("’£", 31256), ("·ºÑ", 31257), ("„É§", 31258), ("ÂÖ∏", 31259), ("Â∫ú", 31260), ("ÃÑ", 31261), ("Ïã†", 31262), ("ÁªÑ", 31263), ("Êîπ", 31264), ("·Ω≤", 31265),("Âçé", 31266), ("‰∏é", 31267), ("Ë∞É", 31268), ("‚ïù", 31269), ("„É¥", 31270), ("·É•", 31271), ("Áî±", 31272), ("‰øÆ", 31273), ("Â≠∏", 31274), ("‚ô£", 31275), ("Ê∂à", 31276), ("Á¨¶", 31277), (" å", 31278), ("Î∂Ä", 31279), ("·ªõ", 31280), ("‚Äæ", 31281), ("‚ñ≤", 31282), ("ÂΩï", 31283), ("‡¥≥", 31284), ("Ïó∞", 31285), ("ÏùÑ", 31286), ("„Å≤", 31287), 
            ("ÏòÅ", 31288), ("‚î§", 31289), ("Â∑≤", 31290), ("ÈôΩ", 31291), ("·ÄÑ", 31292), ("Íµ≠", 31293), ("ÂÆπ", 31294), ("Êú™", 31295), ("ÂÆó", 31296), ("·¥á", 31297), ("„Å≥", 31298), ("Ïû•", 31299), ("Èæô", 31300), ("‡∑ä", 31301), ("Êèê", 31302), ("ƒù", 31303), ("ÂÖ≠", 31304), ("ÂΩ¢", 31305), ("Ï†ú", 31306), ("’Ä", 31307), ("‰ºä", 31308), ("œµ", 31309), ("‡∏Ç", 31310), ("≈∞", 31311), ("„ÇÉ", 31312), ("ÁÅ´", 31313), ("·π¢", 31314), ("‰Ωê", 31315), ("‚ä•", 31316), ("Ã™", 31317), ("·ª©", 31318), ("‚ñ°", 31319), ("Áªì", 31320), ("‰πù", 31321), ("ÈõÑ", 31322), ("’©", 31323), ("·û∂", 31324), ("ËÄå", 31325), ("‡Ωñ", 31326), ("Ïö∞", 31327), ("Âº†", 31328), ("‡§ü", 31329), ("‡§∑", 31330), ("Âêë", 31331), ("·ø•", 31332), ("ÈÄâ", 31333), 
            ("Í≥µ", 31334), ("„Ç≤", 31335), (" ê", 31336), ("‰ªÅ", 31337), ("Â†Ç", 31338), ("◊ö", 31339), ("·ÄØ", 31340), ("·ºî", 31341), ("‡¥Ö", 31342), ("·ªÅ", 31343), ("‡Ωë", 31344), ("ÏÑ†", 31345), ("Ïò§", 31346), ("‰πÖ", 31347), ("¬ú", 31348), ("‰πâ", 31349), ("‡§Ö", 31350), ("‚ïî", 31351), ("Êó†", 31352), ("", 31353), ("ÏùÄ", 31354), (" ∑", 31355), ("ÈÇ£", 31356), ("Á∑ö", 31357), ("Âä°", 31358), ("Âü∫", 31359), ("Â±û", 31360), ("ÈÖç", 31361), ("ÎØ∏", 31362), ("Ëªç", 31363), ("‡πÇ", 31364), ("Ê¥•", 31365), ("ÂÆå", 31366), ("Á†î", 31367), ("Ê≥®", 31368), ("Â§±", 31369), ("Â∫î", 31370), ("·ÄÄ", 31371), ("‚ïö", 31372), ("Âèã", 31373), ("Á´†", 31374), ("Œ®", 31375), ("Ê±Ç", 31376), ("‡§£", 31377), ("Í≤Ω", 31378),  ("‡§≠", 31380), 
            ("‰ª¨", 31381), ("Ê®°", 31382), ("ÈúÄ", 31383), ("‡Æö", 31384), ("Èõª", 31385), ("‡¶™", 31386), ("’§", 31387), ("„Å∏", 31388), ("Ê≠§", 31389), ("Â§ú", 31390), ("Êàñ", 31391), ("Ê©ã", 31392), ("Ê†π", 31393), ("ƒ™", 31394), ("Áéâ", 31395), ("‡∏π", 31396), ("·πÖ", 31397), ("‰∫§", 31398), ("ÂìÅ", 31399), ("ËâØ", 31400), ("‡ΩÑ", 31401), ("„Ç©", 31402), ("Âàô", 31403), ("Èñã", 31404), ("Œñ", 31405), ("Î¨∏", 31406), ("Ë¢´", 31407), ("Ï°∞", 31408), ("Ê†™", 31409), ("ËÆ∞", 31410), ("ÊúÉ", 31411), ("Áªè", 31412), ("‡•Ç", 31413), ("„Çá", 31414), ("ËΩ¨", 31415), ("Â¥é", 31416), ("Îßà", 31417), ("‚åò", 31418), ("ÊØî", 31419), ("ÈÄ†", 31420), ("‹ê", 31421), ("‡∏∑", 31422), ("Ê≤°", 31423), ("Áé∞", 31424), ("‰∏É", 31425), ("ŒÜ", 31426), 
            ("ÂïÜ", 31427), ("‡Øà", 31428), ("Êú∫", 31429), ("Èò≥", 31430), ("ƒâ", 31431), ("Ëßí", 31432), ("Á´ô", 31433), ("’¢", 31434), ("Ìï¥", 31435), ("Âèä", 31436),("‡§ß", 31437), ("Ë°ì", 31438), ("ËÆ§", 31439), ("¬ë", 31440), ("Âàõ", 31441), ("Á∑®", 31442), ("’≤", 31443), ("·∏©", 31444), ("‰ºù", 31445), ("Â≤°", 31446), ("‡§°", 31447), ("„Éõ", 31448), ("Ê∏Ø", 31449), ("‰ªª", 31450), ("Áôª", 31451), ("‡Ω≤", 31452), ("‡πá", 31453), ("Â∏É", 31454), ("Á©∂", 31455), ("Â∏ù", 31456), ("Ïó¨", 31457), ("ÏÇ∞", 31458), ("·Äî", 31459), ("‚ó¶", 31460), ("ÂØÜ", 31461), ("Âèò", 31462), ("Â∫è", 31463), ("‚ôÄ", 31464), ("‚à£", 31465), ("ËÆ°", 31466), ("Êõ≤", 31467), ("ƒÇ", 31468), ("·Ωª", 31469), (" ã", 31470), ("‰º†", 31471), ("„Äë", 31472), ("ÂåÖ", 31473),
            ("ÊÑè", 31474), ("Âéª", 31475), ("Ê≤ô", 31476), ("‚∏Æ", 31477), ("„Äê", 31478), ("ÂÜô", 31479), ("Ë∂Ö", 31480), ("‡ÆØ", 31481), ("‰ªä", 31482), ("‚îà", 31483), ("Ê£Æ", 31484), ("‡∑í", 31485), ("‚äó", 31486), ("ÎπÑ", 31487), ("’∞", 31488), ("·∏®", 31489), ("«´", 31490), ("ÈªÑ", 31491), ("‚àô", 31492), ("Îìú", 31493), ("üåç", 31494), ("ÊôØ", 31495), ("Êπñ", 31496), ("÷Ñ", 31497), ("·Ä≠", 31498), ("‚Åø", 31499), ("ÃÇ", 31500), ("„Éö", 31501), ("‰Ωï", 31502), ("ÂÆá", 31503), ("Âºµ", 31504), ("ËØ≠", 31505), ("ËÄÅ", 31506), ("‰æã", 31507), ("·π¨", 31508), ("ÈâÑ", 31509), ("ÂÖã", 31510), ("‚òâ", 31511), ("¬ô", 31512), ("…π", 31513), ("·º±", 31514), ("‚¥∞", 31515), ("ÁÑ∂", 31516), ("Î•º", 31517), ("«ß", 31518), ("Â†±", 31519), ("Êúç", 31520),
            ("ƒé", 31521), ("ÊÉ≥", 31522), ("‚Äñ", 31523), ("„É¶", 31524), ("ÂÆü", 31525), ("ËΩΩ", 31526),
            ),
            (('Í∑∏', 31607),('‡∂±', 31609),('‡Ωì', 31614),),
            (('„ÇÜ', 31621),('„Åî', 31622),('ÌòÑ', 31680),),
            (('Íµ∞', 31699), ('Î¨¥', 31716), ('ÏúÑ', 31724),),
            (('Ïïà', 31734), ('Î∞ï', 31736),),
            (('Ïö©', 31737), ('Îã®', 31746),),
            (('Î©¥', 31747), ('ÎÇ®', 31754),),
            (('Í∞ï', 31774), ('Ïî®', 31781),),
            (('Í∞ú', 31789), ('Îì§', 31804),),
            (('Ï∞®', 31817), ('Ìïô', 31822), ('Îßå', 31826), ('ÌÑ∞', 31856), ('Ïãù', 31895), ('Í≥º', 31906), ('ÌÉÄ', 31925), ('Ï¢Ö', 31930), ('ÎÇ¥', 31940), ('Ï§ë', 31941), ('Î∞©', 31945)),
            (('Ïõî', 31950), ('Ìöå', 31953), ('Î™®', 31962), ('Î∞î', 31963), ('Ïùå', 31966), ('Ïû¨', 31973), ('Î™Ö', 31976), ('Ìï©', 31980), ('Ïó≠', 31987), ('Î∞±', 31989), ('Ïôï', 31996)),
        ]
    elif tokenizer_type == "mistral":
        special_tokens = [
            (('Êú±', 31947),('«ù', 31948),('·∏®', 31949),('ÊãÖ', 31950),('ÁÅ∞', 31951), ('ËÆ≤', 31952), ('Î°§', 31953),('üò§', 31955),('·üÑ', 31956),('Ïï†', 31957),),
            (('ÏòÄ', 31958),('Ïßà', 31959),('ÊåØ', 31960),),
            (('ÁÅØ', 31961),('ƒâ', 31962),('‡∑É', 31963),),
            (('Èñâ', 31964),('Îû®', 31965),('‡≤Ç', 31966),),
            (('„Åí', 31967),('„Åµ', 31896),),
            (('ÁãÇ', 31969),('Ëûç', 31970),),
            (('‰ªç', 31971),('ÂØ¶', 31972),),
            (('Ê•Ω', 31973),('ÁØÑ', 31974),),
            (('‡∞µ', 31976),('Âµå', 31977),),
            (('Êë©', 31978),('Ë¢Å', 31979),('‡¶∑', 31980),('‰πé', 31981),('Í∑ú', 31982),('Â≤ó', 31983),('Á≥ä', 31984),('‡∞ï', 31985),('Èõ≤', 31986),('Ïã¨', 31987),('‡§à', 31988),('Â∫≠', 31926), ('Ëãó', 31927),('Èó≤', 31929), ('ÎèÖ', 31930), ('…π', 31931), ('“Ω', 31932), ('·ûê', 31933), ('ÂÆè', 31934), ('Â∞ä', 31935), ('Á∏Ω', 31936),),
            (('‡Ω†', 31989),('·º°', 31990),('‰∏ù', 31991),('ƒ¶', 31992),('Ÿç', 31993),('Ÿì', 31994),('·Ä°', 31995),('Âü∑', 31996),('Î≤®', 31997),('„Çº', 31998),('Ê¢¶', 31999), ('Ë£ù', 31937), ('‡∂∏', 31938), ('‚ñ∏', 31939), ('Ê∏¨', 31940), ('Âãá', 31920), ('Âæê', 31921), ('ËΩ©', 31943), ('ÂÖÑ', 31944), ('Ââë', 31945), ('‡™®', 31946),),
        ]
    elif tokenizer_type == "llama-3":
        special_tokens = [
            (('<|reserved_special_token_180|>', 128185), ('<|reserved_special_token_181|>', 128186), ('<|reserved_special_token_182|>', 128187), ('<|reserved_special_token_183|>', 128188), ('<|reserved_special_token_184|>', 128189), ('<|reserved_special_token_185|>', 128190), ('<|reserved_special_token_186|>', 128191), ('<|reserved_special_token_187|>', 128192), ('<|reserved_special_token_188|>', 128193), ('<|reserved_special_token_189|>', 128194), ('<|reserved_special_token_5|>', 128010), ('<|reserved_special_token_6|>', 128011), ('<|reserved_special_token_7|>', 128012), ('<|reserved_special_token_8|>', 128013), ('<|reserved_special_token_9|>', 128014), ('<|reserved_special_token_10|>', 128015), ('<|reserved_special_token_11|>', 128016), ('<|reserved_special_token_12|>', 128017), ('<|reserved_special_token_13|>', 128018), ('<|reserved_special_token_14|>', 128019), ('<|reserved_special_token_15|>', 128020), 
             ('<|reserved_special_token_16|>', 128021), ('<|reserved_special_token_17|>', 128022), ('<|reserved_special_token_18|>', 128023), ('<|reserved_special_token_19|>', 128024), ('<|reserved_special_token_20|>', 128025), ('<|reserved_special_token_21|>', 128026), ('<|reserved_special_token_22|>', 128027), ('<|reserved_special_token_23|>', 128028), ('<|reserved_special_token_24|>', 128029), ('<|reserved_special_token_25|>', 128030), ('<|reserved_special_token_26|>', 128031), ('<|reserved_special_token_27|>', 128032), ('<|reserved_special_token_28|>', 128033), ('<|reserved_special_token_29|>', 128034), ('<|reserved_special_token_30|>', 128035), ('<|reserved_special_token_31|>', 128036), ('<|reserved_special_token_32|>', 128037), ('<|reserved_special_token_33|>', 128038), ('<|reserved_special_token_34|>', 128039), ('<|reserved_special_token_35|>', 128040), ('<|reserved_special_token_36|>', 128041), 
             ('<|reserved_special_token_37|>', 128042), ('<|reserved_special_token_38|>', 128043), ('<|reserved_special_token_39|>', 128044), ('<|reserved_special_token_40|>', 128045), ('<|reserved_special_token_41|>', 128046), ('<|reserved_special_token_42|>', 128047), ('<|reserved_special_token_43|>', 128048), ('<|reserved_special_token_44|>', 128049), ('<|reserved_special_token_45|>', 128050), ('<|reserved_special_token_46|>', 128051), ('<|reserved_special_token_47|>', 128052), ('<|reserved_special_token_48|>', 128053), ('<|reserved_special_token_49|>', 128054), ('<|reserved_special_token_50|>', 128055), ('<|reserved_special_token_51|>', 128056), ('<|reserved_special_token_52|>', 128057), ('<|reserved_special_token_53|>', 128058), ('<|reserved_special_token_54|>', 128059), ('<|reserved_special_token_55|>', 128060), ('<|reserved_special_token_56|>', 128061), ('<|reserved_special_token_57|>', 128062), 
             ('<|reserved_special_token_58|>', 128063), ('<|reserved_special_token_59|>', 128064), ('<|reserved_special_token_60|>', 128065), ('<|reserved_special_token_61|>', 128066), ('<|reserved_special_token_62|>', 128067), ('<|reserved_special_token_63|>', 128068), ('<|reserved_special_token_64|>', 128069), ('<|reserved_special_token_65|>', 128070), ('<|reserved_special_token_66|>', 128071), ('<|reserved_special_token_67|>', 128072), ('<|reserved_special_token_68|>', 128073), ('<|reserved_special_token_69|>', 128074), ('<|reserved_special_token_70|>', 128075), ('<|reserved_special_token_71|>', 128076), ('<|reserved_special_token_72|>', 128077), ('<|reserved_special_token_73|>', 128078), ('<|reserved_special_token_74|>', 128079), ('<|reserved_special_token_75|>', 128080), ('<|reserved_special_token_76|>', 128081), ('<|reserved_special_token_77|>', 128082), ('<|reserved_special_token_78|>', 128083), 
             ('<|reserved_special_token_79|>', 128084), ('<|reserved_special_token_80|>', 128085), ('<|reserved_special_token_81|>', 128086), ('<|reserved_special_token_82|>', 128087), ('<|reserved_special_token_83|>', 128088), ('<|reserved_special_token_84|>', 128089), ('<|reserved_special_token_85|>', 128090), ('<|reserved_special_token_86|>', 128091), ('<|reserved_special_token_87|>', 128092), ('<|reserved_special_token_88|>', 128093), ('<|reserved_special_token_89|>', 128094), ('<|reserved_special_token_90|>', 128095), ('<|reserved_special_token_91|>', 128096), ('<|reserved_special_token_92|>', 128097), ('<|reserved_special_token_93|>', 128098), ('<|reserved_special_token_94|>', 128099), ('<|reserved_special_token_95|>', 128100), ('<|reserved_special_token_96|>', 128101), ('<|reserved_special_token_97|>', 128102), ('<|reserved_special_token_98|>', 128103), ('<|reserved_special_token_99|>', 128104), 
             ('<|reserved_special_token_100|>', 128105), ('<|reserved_special_token_101|>', 128106), ('<|reserved_special_token_102|>', 128107), ('<|reserved_special_token_103|>', 128108), ('<|reserved_special_token_104|>', 128109), ('<|reserved_special_token_105|>', 128110), ('<|reserved_special_token_106|>', 128111), ('<|reserved_special_token_107|>', 128112), ('<|reserved_special_token_108|>', 128113), ('<|reserved_special_token_109|>', 128114), ('<|reserved_special_token_110|>', 128115), ('<|reserved_special_token_111|>', 128116), ('<|reserved_special_token_112|>', 128117), ('<|reserved_special_token_113|>', 128118), ('<|reserved_special_token_114|>', 128119), ('<|reserved_special_token_115|>', 128120), ('<|reserved_special_token_116|>', 128121), ('<|reserved_special_token_117|>', 128122), ('<|reserved_special_token_118|>', 128123), ('<|reserved_special_token_119|>', 128124), ('<|reserved_special_token_120|>', 128125), 
             ('<|reserved_special_token_121|>', 128126), ('<|reserved_special_token_122|>', 128127), ('<|reserved_special_token_123|>', 128128), ('<|reserved_special_token_124|>', 128129), ('<|reserved_special_token_125|>', 128130), ('<|reserved_special_token_126|>', 128131), ('<|reserved_special_token_127|>', 128132), ('<|reserved_special_token_128|>', 128133), ('<|reserved_special_token_129|>', 128134), ('<|reserved_special_token_130|>', 128135), ('<|reserved_special_token_131|>', 128136), ('<|reserved_special_token_132|>', 128137), ('<|reserved_special_token_133|>', 128138), ('<|reserved_special_token_134|>', 128139), ('<|reserved_special_token_135|>', 128140), ('<|reserved_special_token_136|>', 128141), ('<|reserved_special_token_137|>', 128142), ('<|reserved_special_token_138|>', 128143), ('<|reserved_special_token_139|>', 128144), ('<|reserved_special_token_140|>', 128145), ('<|reserved_special_token_141|>', 128146), 
             ('<|reserved_special_token_142|>', 128147), ('<|reserved_special_token_143|>', 128148), ('<|reserved_special_token_144|>', 128149), ('<|reserved_special_token_145|>', 128150), ('<|reserved_special_token_146|>', 128151), ('<|reserved_special_token_147|>', 128152), ('<|reserved_special_token_148|>', 128153), ('<|reserved_special_token_149|>', 128154), ('<|reserved_special_token_150|>', 128155), ('<|reserved_special_token_151|>', 128156), ('<|reserved_special_token_152|>', 128157), ('<|reserved_special_token_153|>', 128158), ('<|reserved_special_token_154|>', 128159), ('<|reserved_special_token_155|>', 128160), ('<|reserved_special_token_156|>', 128161), ('<|reserved_special_token_157|>', 128162), ('<|reserved_special_token_158|>', 128163), ('<|reserved_special_token_159|>', 128164), ('<|reserved_special_token_160|>', 128165), ('<|reserved_special_token_161|>', 128166), ('<|reserved_special_token_162|>', 128167), 
             ('<|reserved_special_token_163|>', 128168), ('<|reserved_special_token_164|>', 128169), ('<|reserved_special_token_165|>', 128170), ('<|reserved_special_token_166|>', 128171), ('<|reserved_special_token_167|>', 128172), ('<|reserved_special_token_168|>', 128173), ('<|reserved_special_token_169|>', 128174), ('<|reserved_special_token_170|>', 128175), ('<|reserved_special_token_171|>', 128176), ('<|reserved_special_token_172|>', 128177), ('<|reserved_special_token_173|>', 128178), ('<|reserved_special_token_174|>', 128179), ('<|reserved_special_token_175|>', 128180), ('<|reserved_special_token_176|>', 128181), ('<|reserved_special_token_177|>', 128182),
            ),
            (('<|reserved_special_token_190|>', 128195), ('<|reserved_special_token_191|>', 128196), ('<|reserved_special_token_192|>', 128197), ),
            (('<|reserved_special_token_193|>', 128198), ('<|reserved_special_token_194|>', 128199), ('<|reserved_special_token_195|>', 128200), ),
            (('<|reserved_special_token_196|>', 128201), ('<|reserved_special_token_197|>', 128202), ('<|reserved_special_token_198|>', 128203), ),
            (('<|reserved_special_token_199|>', 128204), ('<|reserved_special_token_200|>', 128205),),
            (('<|reserved_special_token_201|>', 128206), ('<|reserved_special_token_202|>', 128207), ),
            (('<|reserved_special_token_203|>', 128208), ('<|reserved_special_token_204|>', 128209), ),
            (('<|reserved_special_token_205|>', 128210), ('<|reserved_special_token_206|>', 128211),),
            (('<|reserved_special_token_207|>', 128212), ('<|reserved_special_token_208|>', 128213), ),
            (('<|reserved_special_token_209|>', 128214), ('<|reserved_special_token_210|>', 128215), ('<|reserved_special_token_211|>', 128216), ('<|reserved_special_token_212|>', 128217), ('<|reserved_special_token_213|>', 128218), ('<|reserved_special_token_214|>', 128219), ('<|reserved_special_token_215|>', 128220), ('<|reserved_special_token_216|>', 128221), ('<|reserved_special_token_217|>', 128222), ('<|reserved_special_token_218|>', 128223), ('<|reserved_special_token_219|>', 128224), ('<|reserved_special_token_220|>', 128225), ('<|reserved_special_token_221|>', 128226), ('<|reserved_special_token_222|>', 128227), ('<|reserved_special_token_223|>', 128228), ('<|reserved_special_token_224|>', 128229), ('<|reserved_special_token_225|>', 128230), ('<|reserved_special_token_226|>', 128231), ('<|reserved_special_token_227|>', 128232), ('<|reserved_special_token_228|>', 128233), ('<|reserved_special_token_229|>', 128234), ),
            (('<|reserved_special_token_230|>', 128235), ('<|reserved_special_token_231|>', 128236), ('<|reserved_special_token_232|>', 128237), ('<|reserved_special_token_233|>', 128238), ('<|reserved_special_token_234|>', 128239), ('<|reserved_special_token_235|>', 128240), ('<|reserved_special_token_236|>', 128241), ('<|reserved_special_token_237|>', 128242), ('<|reserved_special_token_238|>', 128243), ('<|reserved_special_token_239|>', 128244), ('<|reserved_special_token_240|>', 128245), ('<|reserved_special_token_241|>', 128246), ('<|reserved_special_token_242|>', 128247), ('<|reserved_special_token_243|>', 128248), ('<|reserved_special_token_244|>', 128249), ('<|reserved_special_token_245|>', 128250), ('<|reserved_special_token_246|>', 128251), ('<|reserved_special_token_247|>', 128252), ('<|reserved_special_token_248|>', 128253), ('<|reserved_special_token_249|>', 128254), ('<|reserved_special_token_250|>', 128255)),
        ]
    else:
        raise ValueError(f"The tokenizer type {tokenizer_type} is not supported in control tokens.")
    return special_tokens[place][num][not_text]

def remap_control_token(token:str,use_num=True, tokenizer_type:str = "llama-2")->tuple:
    """Áî±tokenÊò†Â∞ÑÂà∞actionÔºåÊ≥®ÊÑèÔºåËôΩÁÑ∂Êääcamera‰ªétoken‰∏≠ÂéªÊéâÔºå‰ΩÜÊòØËøòÈúÄË¶ÅÂÆÉ """
    re_tokens = {}
    if tokenizer_type == "llama-2":
        if use_num:
            re_tokens = {31536: (0, 0), 31537: (0, 1), 31563: (0, 2), 31571: (0, 3), 31578: (0, 4), 31582: (0, 5), 31585: (0, 6), 31598: (0, 7), 
                         31603: (0, 8), 31604: (0, 9), 31000: (0, 10), 31001: (0, 11), 31002: (0, 12), 31003: (0, 13), 31004: (0, 14), 31005: (0, 15), 
                         31006: (0, 16), 31007: (0, 17), 31008: (0, 18), 31009: (0, 19), 31010: (0, 20), 31011: (0, 21), 31012: (0, 22), 31013: (0, 23), 
                         31014: (0, 24), 31015: (0, 25), 31016: (0, 26), 31017: (0, 27), 31018: (0, 28), 31019: (0, 29), 31020: (0, 30), 31021: (0, 31), 31022: (0, 32), 31023: (0, 33), 31024: (0, 34), 31025: (0, 35), 31026: (0, 36), 31027: (0, 37), 
                         31028: (0, 38), 31029: (0, 39), 31030: (0, 40), 31031: (0, 41), 31032: (0, 42), 31033: (0, 43), 31034: (0, 44), 31035: (0, 45), 31036: (0, 46), 31037: (0, 47), 31038: (0, 48), 31039: (0, 49), 31040: (0, 50), 31041: (0, 51), 
                         31042: (0, 52), 31043: (0, 53), 31044: (0, 54), 31045: (0, 55), 31046: (0, 56), 31047: (0, 57), 31048: (0, 58), 31049: (0, 59), 31050: (0, 60), 31051: (0, 61), 31052: (0, 62), 31053: (0, 63), 31054: (0, 64), 31055: (0, 65), 
                         31056: (0, 66), 31057: (0, 67), 31058: (0, 68), 31059: (0, 69), 31060: (0, 70), 31061: (0, 71), 31062: (0, 72), 31063: (0, 73), 31064: (0, 74), 31065: (0, 75), 31066: (0, 76), 31067: (0, 77), 31068: (0, 78), 31069: (0, 79), 
                         31070: (0, 80), 31071: (0, 81), 31072: (0, 82), 31073: (0, 83), 31074: (0, 84), 31075: (0, 85), 31076: (0, 86), 31077: (0, 87), 31078: (0, 88), 31079: (0, 89), 31080: (0, 90), 31081: (0, 91), 31082: (0, 92), 31083: (0, 93), 
                         31084: (0, 94), 31085: (0, 95), 31086: (0, 96), 31087: (0, 97), 31088: (0, 98), 31089: (0, 99), 31090: (0, 100), 31091: (0, 101), 31092: (0, 102), 31093: (0, 103), 31094: (0, 104), 31095: (0, 105), 31096: (0, 106), 31097: (0, 107), 
                         31098: (0, 108), 31099: (0, 109), 31100: (0, 110), 31101: (0, 111), 31102: (0, 112), 31103: (0, 113), 31104: (0, 114), 31105: (0, 115), 31106: (0, 116), 31107: (0, 117), 31108: (0, 118), 31109: (0, 119), 31110: (0, 120), 31111: (0, 121), 
                         31112: (0, 122), 31113: (0, 123), 31114: (0, 124), 31115: (0, 125), 31116: (0, 126), 31117: (0, 127), 31118: (0, 128), 31119: (0, 129), 31120: (0, 130), 31121: (0, 131), 31122: (0, 132), 31123: (0, 133), 31124: (0, 134), 31125: (0, 135), 31126: (0, 136), 31127: (0, 137), 31128: (0, 138), 31129: (0, 139), 31130: (0, 140), 31131: (0, 141), 31132: (0, 142), 31133: (0, 143), 31134: (0, 144), 31135: (0, 145), 31136: (0, 146), 31137: (0, 147), 31138: (0, 148), 31139: (0, 149), 
                         31140: (0, 150), 31141: (0, 151), 31142: (0, 152), 31143: (0, 153), 31144: (0, 154), 31145: (0, 155), 31146: (0, 156), 31147: (0, 157), 31148: (0, 158), 31149: (0, 159), 31150: (0, 160), 31151: (0, 161), 31152: (0, 162), 31153: (0, 163), 31154: (0, 164), 31155: (0, 165), 31156: (0, 166), 31157: (0, 167), 31158: (0, 168), 31159: (0, 169), 31160: (0, 170), 31161: (0, 171), 31162: (0, 172), 31163: (0, 173), 31164: (0, 174), 31165: (0, 175), 31166: (0, 176), 31167: (0, 177), 31168: (0, 178), 
                         31169: (0, 179), 31170: (0, 180), 31171: (0, 181), 31172: (0, 182), 31173: (0, 183), 31174: (0, 184), 31175: (0, 185), 31176: (0, 186), 31177: (0, 187), 31178: (0, 188), 31179: (0, 189), 31180: (0, 190), 31181: (0, 191), 31182: (0, 192), 31183: (0, 193), 31184: (0, 194), 31185: (0, 195), 31186: (0, 196), 31187: (0, 197), 31188: (0, 198), 31189: (0, 199), 31190: (0, 200), 31191: (0, 201), 31192: (0, 202), 31193: (0, 203), 31195: (0, 204), 31196: (0, 205), 31197: (0, 206), 31198: (0, 207), 
                         31199: (0, 208), 31200: (0, 209), 31201: (0, 210), 31202: (0, 211), 31203: (0, 212), 31204: (0, 213), 31205: (0, 214), 31206: (0, 215), 31207: (0, 216), 31208: (0, 217), 31209: (0, 218), 31210: (0, 219), 31211: (0, 220), 31212: (0, 221), 31213: (0, 222), 31214: (0, 223), 31215: (0, 224), 31216: (0, 225), 31217: (0, 226), 31218: (0, 227), 31219: (0, 228), 31220: (0, 229), 31221: (0, 230), 31222: (0, 231), 31223: (0, 232), 31224: (0, 233), 31225: (0, 234), 31226: (0, 235), 31227: (0, 236), 
                         31228: (0, 237), 31229: (0, 238), 31230: (0, 239), 31231: (0, 240), 31232: (0, 241), 31233: (0, 242), 31234: (0, 243), 31235: (0, 244), 31236: (0, 245), 31237: (0, 246), 31238: (0, 247), 31239: (0, 248), 31240: (0, 249), 31241: (0, 250), 31243: (0, 251), 31244: (0, 252), 31245: (0, 253), 31246: (0, 254), 31247: (0, 255), 31248: (0, 256), 31249: (0, 257), 31250: (0, 258), 31251: (0, 259), 31252: (0, 260), 31253: (0, 261), 31254: (0, 262), 31255: (0, 263), 31256: (0, 264), 31257: (0, 265), 
                         31258: (0, 266), 31259: (0, 267), 31260: (0, 268), 31261: (0, 269), 31262: (0, 270), 31263: (0, 271), 31264: (0, 272), 31265: (0, 273), 31266: (0, 274), 31267: (0, 275), 31268: (0, 276), 31269: (0, 277), 31270: (0, 278), 31271: (0, 279), 31272: (0, 280), 31273: (0, 281), 31274: (0, 282), 31275: (0, 283), 31276: (0, 284), 31277: (0, 285), 31278: (0, 286), 31279: (0, 287), 31280: (0, 288), 31281: (0, 289), 31282: (0, 290), 31283: (0, 291), 31284: (0, 292), 31285: (0, 293), 31286: (0, 294), 
                         31287: (0, 295), 31288: (0, 296), 31289: (0, 297), 31290: (0, 298), 31291: (0, 299), 31292: (0, 300), 31293: (0, 301), 31294: (0, 302), 31295: (0, 303), 31296: (0, 304), 31297: (0, 305), 31298: (0, 306), 31299: (0, 307), 31300: (0, 308), 31301: (0, 309), 31302: (0, 310), 31303: (0, 311), 31304: (0, 312), 31305: (0, 313), 31306: (0, 314), 31307: (0, 315), 31308: (0, 316), 31309: (0, 317), 31310: (0, 318), 31311: (0, 319), 31312: (0, 320), 31313: (0, 321), 31314: (0, 322), 31315: (0, 323), 
                         31316: (0, 324), 31317: (0, 325), 31318: (0, 326), 31319: (0, 327), 31320: (0, 328), 31321: (0, 329), 31322: (0, 330), 31323: (0, 331), 31324: (0, 332), 31325: (0, 333), 31326: (0, 334), 31327: (0, 335), 31328: (0, 336), 31329: (0, 337), 31330: (0, 338), 31331: (0, 339), 31332: (0, 340), 
                         31333: (0, 341), 31334: (0, 342), 31335: (0, 343), 31336: (0, 344), 31337: (0, 345), 31338: (0, 346), 31339: (0, 347), 31340: (0, 348), 31341: (0, 349), 31342: (0, 350), 31343: (0, 351), 31344: (0, 352), 31345: (0, 353), 31346: (0, 354), 31347: (0, 355), 31348: (0, 356), 31349: (0, 357), 31350: (0, 358), 31351: (0, 359), 31352: (0, 360), 31353: (0, 361), 31354: (0, 362), 31355: (0, 363), 31356: (0, 364), 31357: (0, 365), 31358: (0, 366), 31359: (0, 367), 31360: (0, 368), 31361: (0, 369), 
                         31362: (0, 370), 31363: (0, 371), 31364: (0, 372), 31365: (0, 373), 31366: (0, 374), 31367: (0, 375), 31368: (0, 376), 31369: (0, 377), 31370: (0, 378), 31371: (0, 379), 31372: (0, 380), 31373: (0, 381), 31374: (0, 382), 31375: (0, 383), 31376: (0, 384), 31377: (0, 385), 31378: (0, 386), 
                         31380: (0, 387), 31381: (0, 388), 31382: (0, 389), 31383: (0, 390), 31384: (0, 391), 31385: (0, 392), 31386: (0, 393), 31387: (0, 394), 31388: (0, 395), 31389: (0, 396), 31390: (0, 397), 31391: (0, 398), 31392: (0, 399), 31393: (0, 400), 31394: (0, 401), 31395: (0, 402), 31396: (0, 403), 31397: (0, 404), 31398: (0, 405), 31399: (0, 406), 31400: (0, 407), 31401: (0, 408), 31402: (0, 409), 31403: (0, 410), 31404: (0, 411), 
                         31405: (0, 412), 31406: (0, 413), 31407: (0, 414), 31408: (0, 415), 31409: (0, 416), 31410: (0, 417), 31411: (0, 418), 31412: (0, 419), 31413: (0, 420), 31414: (0, 421), 31415: (0, 422), 31416: (0, 423), 31417: (0, 424), 31418: (0, 425), 31419: (0, 426), 31420: (0, 427), 31421: (0, 428), 31422: (0, 429), 31423: (0, 430), 31424: (0, 431), 31425: (0, 432), 31426: (0, 433), 31427: (0, 434), 31428: (0, 435), 31429: (0, 436), 31430: (0, 437), 31431: (0, 438), 31432: (0, 439), 31433: (0, 440), 
                         31434: (0, 441), 31435: (0, 442), 31436: (0, 443), 31437: (0, 444), 31438: (0, 445), 31439: (0, 446), 31440: (0, 447), 31441: (0, 448), 31442: (0, 449), 31443: (0, 450), 31444: (0, 451), 31445: (0, 452), 31446: (0, 453), 31447: (0, 454), 31448: (0, 455), 31449: (0, 456), 31450: (0, 457), 
                         31451: (0, 458), 31452: (0, 459), 31453: (0, 460), 31454: (0, 461), 31455: (0, 462), 31456: (0, 463), 31457: (0, 464), 31458: (0, 465), 31459: (0, 466), 31460: (0, 467), 31461: (0, 468), 31462: (0, 469), 31463: (0, 470), 31464: (0, 471), 31465: (0, 472), 31466: (0, 473), 31467: (0, 474), 31468: (0, 475), 31469: (0, 476), 31470: (0, 477), 31471: (0, 478), 31472: (0, 479), 31473: (0, 480), 31474: (0, 481), 31475: (0, 482), 31476: (0, 483), 31477: (0, 484), 31478: (0, 485), 31479: (0, 486), 
                         31480: (0, 487), 31481: (0, 488), 31482: (0, 489), 31483: (0, 490), 31484: (0, 491), 31485: (0, 492), 31486: (0, 493), 31487: (0, 494), 31488: (0, 495), 31489: (0, 496), 31490: (0, 497), 31491: (0, 498), 31492: (0, 499), 31493: (0, 500), 31494: (0, 501), 31495: (0, 502), 31496: (0, 503), 31497: (0, 504), 31498: (0, 505), 31499: (0, 506), 31500: (0, 507), 31501: (0, 508), 31502: (0, 509), 31503: (0, 510), 31504: (0, 511)}
        else:
            re_tokens = {
                'ÏßÑ': (0, 0),'‡¶ú': (0, 1),'Ï≤ú': (0, 2),'ÎÖÑ': (0, 3),'ÏÑ∏': (0, 4),'ÎØº': (0, 5),'‡µº': (0, 6),'·º°': (0, 7),'Ìò∏': (0, 8),'‡®∞': (0, 9),
                'Í∑∏': (1, 0),'‡∂±': (1, 1),'‡Ωì': (1, 2),
                '„ÇÜ': (2, 0),'„Åî': (2, 1),'ÌòÑ': (2, 2),
                'Íµ∞': (3, 0),'Î¨¥': (3, 1),'ÏúÑ': (3, 2),
                'Ïïà': (4, 0),'Î∞ï': (4, 1),
                'Ïö©': (5, 0),'Îã®': (5, 1),
                'Î©¥': (6, 0),'ÎÇ®': (6, 1),
                'Í∞ï': (7, 0),'Ïî®': (7, 1),
                'Í∞ú': (8, 0),'Îì§': (8, 1),
                'Ï∞®': (9, 0),'Ìïô': (9, 1),'Îßå': (9, 2),'ÌÑ∞': (9, 3),'Ïãù': (9, 4),'Í≥º': (9, 5),'ÌÉÄ': (9, 6),'Ï¢Ö': (9, 7),'ÎÇ¥': (9, 8),'Ï§ë': (9, 9),'Î∞©': (9, 10),
                'Ïõî': (10, 0),'Ìöå': (10, 1),'Î™®': (10, 2),'Î∞î': (10, 3),'Ïùå': (10, 4),'Ïû¨': (10, 5),'Î™Ö': (10, 6),'Ìï©': (10, 7),'Ïó≠': (10, 8),'Î∞±': (10, 9),'Ïôï': (10, 10)
            }
    elif tokenizer_type=="mistral":
        if use_num:
            re_tokens = {31947: (0, 0), 31948: (0, 1), 31949: (0, 2), 31950: (0, 3), 31951: (0, 4), 31952: (0, 5), 31953: (0, 6), 31955: (0, 7), 31956: (0, 8), 31957: (0, 9), 31958: (1, 0), 31959: (1, 1), 31960: (1, 2), 31961: (2, 0), 31962: (2, 1), 31963: (2, 2), 31964: (3, 0), 31965: (3, 1), 31966: (3, 2), 31967: (4, 0), 31896: (4, 1), 31969: (5, 0), 31970: (5, 1), 31971: (6, 0), 31972: (6, 1), 31973: (7, 0), 31974: (7, 1), 31976: (8, 0), 31977: (8, 1), 31978: (9, 0), 31979: (9, 1), 31980: (9, 2), 31981: (9, 3), 31982: (9, 4), 31983: (9, 5), 31984: (9, 6), 31985: (9, 7), 31986: (9, 8), 31987: (9, 9), 31988: (9, 10), 31926: (9, 11), 31927: (9, 12), 31929: (9, 13), 31930: (9, 14), 31931: (9, 15), 31932: (9, 16), 31933: (9, 17), 31934: (9, 18), 31935: (9, 19), 31936: (9, 20), 31989: (10, 0), 31990: (10, 1), 31991: (10, 2), 31992: (10, 3), 31993: (10, 4), 31994: (10, 5), 31995: (10, 6), 31996: (10, 7), 31997: (10, 8), 31998: (10, 9), 31999: (10, 10), 31937: (10, 11), 31938: (10, 12), 31939: (10, 13), 31940: (10, 14), 31920: (10, 15), 31921: (10, 16), 31943: (10, 17), 31944: (10, 18), 31945: (10, 19), 31946: (10, 20)}
        else:
            re_tokens = {
                'Êú±': (0, 0),'«ù': (0, 1),'·∏®': (0, 2),'ÊãÖ': (0, 3),'ÁÅ∞': (0, 4),'ËÆ≤': (0, 5),'Î°§': (0, 6),'üò§': (0, 7),'·üÑ': (0, 8),'Ïï†': (0, 9),
                'ÏòÄ': (1, 0),'Ïßà': (1, 1),'ÊåØ': (1, 2),
                'ÁÅØ': (2, 0),'ƒâ': (2, 1),'‡∑É': (2, 2),
                'Èñâ': (3, 0),'Îû®': (3, 1),'‡≤Ç': (3, 2),
                '„Åí': (4, 0),'„Åµ': (4, 1),
                'ÁãÇ': (5, 0),'Ëûç': (5, 1),
                '‰ªç': (6, 0),'ÂØ¶': (6, 1),
                'Ê•Ω': (7, 0),'ÁØÑ': (7, 1),
                '‡∞µ': (8, 0),'Âµå': (8, 1),
                'Êë©': (9, 0),'Ë¢Å': (9, 1),'‡¶∑': (9, 2),'‰πé': (9, 3),'Í∑ú': (9, 4),'Â≤ó': (9, 5),'Á≥ä': (9, 6),'‡∞ï': (9, 7),'Èõ≤': (9, 8),'Ïã¨': (9, 9),'‡§à': (9, 10),'Â∫≠': (9, 11), 'Ëãó': (9, 12), 'Èó≤': (9, 13), 'ÎèÖ': (9, 14),'…π': (9, 15), '“Ω': (9, 16), '·ûê': (9, 17), 'ÂÆè': (9, 18), 'Â∞ä': (9, 19),'Á∏Ω': (9, 20),
                '‡Ω†': (10, 0),'·º°': (10, 1),'‰∏ù': (10, 2),'ƒ¶': (10, 3),'‰ºù': (10, 4),'Ïª®': (10, 5),'·Ä°': (10, 6),'Âü∑': (10, 7),'Î≤®': (10, 8),'„Çº': (10, 9),'Ê¢¶': (10, 10),'Ë£ù': (10, 11), '‡∂∏': (10, 12), '‚ñ∏': (10, 13), 'Ê∏¨': (10, 14),'Âãá': (10, 15), 'Âæê': (10, 16), 'ËΩ©': (10, 17), 'ÂÖÑ': (10, 18), 'Ââë': (10, 19),'‡™®': (10, 20)
            }
    elif tokenizer_type=="llama-3":
        if use_num:
            re_tokens={128185: (0, 0), 128186: (0, 1), 128187: (0, 2), 128188: (0, 3), 128189: (0, 4), 128190: (0, 5), 128191: (0, 6), 128192: (0, 7), 128193: (0, 8), 128194: (0, 9), 128195: (1, 0), 128196: (1, 1), 128197: (1, 2), 128198: (2, 0), 128199: (2, 1), 128200: (2, 2), 128201: (3, 0), 128202: (3, 1), 128203: (3, 2), 128204: (4, 0), 128205: (4, 1), 128206: (5, 0), 128207: (5, 1), 128208: (6, 0), 128209: (6, 1), 128210: (7, 0), 128211: (7, 1), 128212: (8, 0), 128213: (8, 1), 128214: (9, 0), 128215: (9, 1), 128216: (9, 2), 128217: (9, 3), 128218: (9, 4), 128219: (9, 5), 128220: (9, 6), 128221: (9, 7), 128222: (9, 8), 128223: (9, 9), 128224: (9, 10), 128225: (9, 11), 128226: (9, 12), 128227: (9, 13), 128228: (9, 14), 128229: (9, 15), 128230: (9, 16), 128231: (9, 17), 128232: (9, 18), 128233: (9, 19), 128234: (9, 20), 128235: (10, 0), 128236: (10, 1), 128237: (10, 2), 128238: (10, 3), 128239: (10, 4), 128240: (10, 5), 128241: (10, 6), 128242: (10, 7), 128243: (10, 8), 128244: (10, 9), 128245: (10, 10), 128246: (10, 11), 128247: (10, 12), 128248: (10, 13), 128249: (10, 14), 128250: (10, 15), 128251: (10, 16), 128252: (10, 17), 128253: (10, 18), 128254: (10, 19), 128255: (10, 20)}
        else:
            raise ValueError("can't use text as tokens")
    else:
        raise ValueError(f"The tokenizer type {tokenizer_type} is not supported in control tokens.")
    return re_tokens.get(token,(-1,-1))

def tag_token(place, tokenizer_type:str = "llama-2",return_type:int=0):
    """ÂºïÂÖ•Â§¥Ê†áËÆ∞ÂíåÂ∞æÊ†áËÆ∞ """
    assert place in {0,1}
    if tokenizer_type == "llama-2":
        special_tokens = [('Ïú†', 31533),('Ïöî', 31527)]
    elif tokenizer_type == "mistral":
        special_tokens = [('‡≤Æ', 31941),('·ä†', 31942)]
    elif tokenizer_type=="llama-3":
        special_tokens = [('<|reserved_special_token_178|>', 128183), ('<|reserved_special_token_179|>', 128184),]
    else:
        raise ValueError(f"The tokenizer type {tokenizer_type} is not supported in control tokens.")
    return special_tokens[place][return_type]


from abc import ABC, abstractmethod
class ActionTokenizer:
    movements = ('forward', 'back', 'left', 'right', 'sprint', 'sneak')
    operations = ('use', 'drop', 'attack', 'jump')
    def __init__(self,tokenizer_type="llama-3",
                 camera_quantization_scheme="mu_law",
                 camera_mu=20,
                 n_camera_bins=21,
                 camera_binsize=1):
        self.tokenizer_type = tokenizer_type
        
        
        self.act_beg_id = tag_token(0,self.tokenizer_type,return_type=1)
        self.act_end_id = tag_token(1,self.tokenizer_type,return_type=1)
        
        self.act_beg_token = tag_token(0,self.tokenizer_type,return_type=0)
        self.act_end_token = tag_token(1,self.tokenizer_type,return_type=0)
        
        self.n_camera_bins = n_camera_bins
        self.null_action = {'forward': False, 'back': False, 'left': False, 'right': False, 'sprint': False, 'sneak': False,
                            'hotbar.1': False, 'hotbar.2': False, 'hotbar.3': False, 'hotbar.4': False, 'hotbar.5': False, 'hotbar.6': False, 'hotbar.7': False, 'hotbar.8': False, 'hotbar.9': False, 
                            'use': False, 'drop': False, 'attack': False, 'jump': False, 
                            'inventory': False, 
                            'camera': (0.0, 0.0)}
        
        self.action_transformer = ActionTransformer(camera_maxval=10,camera_quantization_scheme=camera_quantization_scheme,camera_mu=camera_mu,camera_binsize=camera_binsize)
        self.action_mapper = CameraHierarchicalMapping(n_camera_bins=n_camera_bins)
    
        
    
    @abstractmethod
    def encode(self,actions:Dict)->Union[torch.Tensor,list,str]:
        """ 
        encode from actions to tokens
        """
        pass
    
    @abstractmethod
    def decode(self,tokens:Union[torch.Tensor,list])->List[OrderedDict]:
        """ 
        decode from tokens to actions
        tokens: should be Tensor or list, string is forbidden
        """
        pass

class OneActionTokenizer(ActionTokenizer):
    BUTTONS_GROUPS = [
        "hotbar", "fore or back", "left or right", "sprint or sneak", "use", "drop", "attack", "jump", "camera"
    ]
    
    def __init__(self,tokenizer_type="llama-2",bases=[10,3,3,3,2,2,2,2,2,21,21],
                 camera_quantization_scheme="mu_law",
                 camera_mu=20,
                 n_camera_bins=21,
                 camera_binsize=1):
        
        super().__init__(tokenizer_type=tokenizer_type,
                         camera_quantization_scheme=camera_quantization_scheme,
                         camera_mu=camera_mu,
                         n_camera_bins=n_camera_bins,
                         camera_binsize=camera_binsize)
        
        console.Console().log(f"bases: {bases}, camera_mu: {camera_mu}, n_camera_bins: {n_camera_bins}, camera_binsize: {camera_binsize}")
        self.bases = bases
        self.NULL_ACTION = [0,(bases[-2]//2)*bases[-2]+(bases[-1]//2)]        
    
    def decode(self,tokens:Union[torch.Tensor,List]):
        """decode the tokens to action
        """
        group_actions = self.token_2_group_action(tokens,)
        
        actions = [self.group_action_2_decimal_action(group_action) for group_action in group_actions ]
        action_dicts = []
        for action in  actions:
            action_dict = {
                "buttons":np.array([action[0]]),
                "camera":np.array([action[1]]),  #ËøîÂõû‰∏Ä‰∏™Â∑•‰Ωú
            }
            action_dict = OrderedDict({key: value[0] for key, value in action_dict.items()})
            action_dicts.append(action_dict)
            
        return action_dicts
    
    def encode(self,trajectory:dict) -> list[tuple[int]]:
        """encode an action to tokens
        action: tuple -- (button,camera)
        output: str, Â§ö‰∏™token
        """
        minerl_actions = trajectory['actions']
        traj_len = len(minerl_actions['attack'])
        observations = trajectory.get('observations',[""]*traj_len)
        uuids = trajectory.get('uuids',[""]*traj_len)
        minerl_action_transformed = {key: np.array(val) for key, val in minerl_actions.items()} #‰ªélistÂà∞ndarray
        minerl_action = self.action_transformer.env2policy(minerl_action_transformed) 
        actions = self.action_mapper.from_factored(minerl_action,if_inventory=False)
        action_list = []
        for idx in range(traj_len):
            action_list.append((actions["buttons"][idx][0],actions["camera"][idx][0]))
        encoded_trajectory = []
        for idx,action in enumerate(action_list):
            control_token = self.encode_action(action)
            encoded_trajectory.append([control_token,[observations[idx]],uuids[idx]])
        return encoded_trajectory
    
    def encode_action(self,action:tuple)->str:
        """encode an action to tokens
        action: tuple -- (button,camera)
        output: str, Â§ö‰∏™token
        """
        assert len(action)==2
        # map to groups of action 
        group_action = self.decimal_action_2_group_action(action)
        # from action to token
        tokens = self.group_action_2_token(group_action)
        
        return tokens
    
    def group_action_2_token(self,group_action):
        zero_include_token_list = [map_control_token(num, i, self.tokenizer_type) for i, num in enumerate(group_action)]
        control_token = ''.join((s for x,s in zip(group_action[:-3],zero_include_token_list[:-3]) if x != 0 )) #cameraÈîÆÂú®ËøôÈáåÊ≤°ÊúâÊÑè‰πâ
        control_token = control_token + "".join((s for s in zero_include_token_list[-2:]))  #cameraÂøÖÈ°ª‰øùÂ≠ò
        tag_control_token = self.act_beg_token + control_token + self.act_end_token
        return tag_control_token
    
    def token_2_group_action(self,tokens:Union[torch.Tensor,list]):
        actions = []
        action_base = [0]*len(self.bases) #ÂàùÂßãÂåñ
        camera_null = [self.bases[-1]//2,self.bases[-2]//2]
        action_base[-2:] = camera_null

        if isinstance(tokens, torch.Tensor):
            # Â¶ÇÊûúÊòØ‰∫åÁª¥Âº†Èáè (shape == 2)ÔºåÂàôÈúÄË¶Å squeeze
            if tokens.ndim == 2:
                tokens = tokens.squeeze()
            tokens = tokens.tolist()
        elif not isinstance(tokens, list):
            raise ValueError("wrong type!")

        start_idx = 0
        while start_idx < len(tokens):
            try:
                first_index_n1 = tokens.index(self.act_beg_id, start_idx)
                first_index_n2 = tokens.index(self.act_end_id, first_index_n1 + 1)
            except ValueError:
                break
            
            control_tokens = tokens[first_index_n1 + 1:first_index_n2]
            action = copy.copy(action_base)
            
            # ÂØπ control_tokens ‰∏≠ÊØè‰∏™ token ËøõË°å remap Âπ∂Êõ¥Êñ∞ action
            for token in control_tokens:
                place, num = remap_control_token(token, use_num=True, tokenizer_type=self.tokenizer_type)
                if place != -1:
                    action[place] = num
            
            if action[-2:] != camera_null:
                action[-3] = 1
            
            actions.append(copy.copy(action))
            start_idx = first_index_n2 + 1

        if len(actions)  == 0:
            actions.append(action_base)

        return actions

    def decimal_action_2_group_action(self,inputs):
        """ 
        Params:
        * output: set, ËøîÂõû‰∏Ä‰∏™ÂÖÉÁªÑ, ÂÖÉÁªÑ‰∏≠ÁöÑÊØè‰∏™ÂÖÉÁ¥†Ë°®Á§∫‰∏Ä‰∏™‰ΩçÁöÑÂÄº
        * inputs: tuple, ‰∏§‰∏™ÂçÅËøõÂà∂Êï¥Êï∞
        * bases: list, ÊØè‰ΩçÁöÑÂü∫Êï∞     

        Function: Â∞Ü‰∏Ä‰∏™ÂçÅËøõÂà∂Êï¥Êï∞ËΩ¨Êç¢‰∏∫ÂÖ∑Êúâ‰∏çÂêåÂü∫Êï∞ÁöÑÊï∞Â≠óÁ≥ªÁªü(ÊØè‰ΩçÁöÑÂü∫Êï∞ÂàÜÂà´‰∏∫ [8, 8, 8, 6, 5]), ÈúÄË¶ÅÁºñÂÜô‰∏Ä‰∏™PythonÂáΩÊï∞Êù•ÊâßË°åÈÄÜÂêëËÆ°ÁÆó„ÄÇËøô‰∏™ËΩ¨Êç¢Ê∂âÂèäÂ∞ÜÂçÅËøõÂà∂Êï∞ÈÄê‰ΩçÈô§‰ª•ÂØπÂ∫îÁöÑÂü∫Êï∞Âπ∂Âèñ‰ΩôÊï∞, ÁÑ∂ÂêéÂÜçÁªßÁª≠Â§ÑÁêÜÂïÜ„ÄÇ
        """
        decimals = list(inputs)
        #decimals[0] = decimals[0]//2 #cameraÈîÆÂú®ËøôÈáåÊ≤°ÊúâÊÑè‰πâ
        # Áî®‰∫éÂ≠òÂÇ®ËΩ¨Êç¢ÁªìÊûúÁöÑÂàóË°®
        result = [0] * len(self.bases)
        # ‰ªéÊúÄ‰Ωé‰ΩçÂà∞ÊúÄÈ´ò‰ΩçÈÄê‰ΩçËÆ°ÁÆó
        for i in range(len(self.bases)-3, -1, -1):
            # Ê±ÇÂΩìÂâç‰ΩçÁöÑÂÄº
            result[i] = decimals[0] % self.bases[i]
            # Êõ¥Êñ∞ÂçÅËøõÂà∂Êï∞‰∏∫‰∏ã‰∏Ä‰ΩçÁöÑÂ§ÑÁêÜ
            decimals[0] //= self.bases[i]
        # Á°Æ‰øùËΩ¨Êç¢ËøáÁ®ã‰∏≠ÂçÅËøõÂà∂Êï∞Ë¢´ÂÆåÂÖ®ËΩ¨Êç¢
        result[-1] = decimals[1] % self.bases[-1]
        decimals[1] //= self.bases[-1]
        result[-2] = decimals[1] % self.bases[-2]
        decimals[1] //= self.bases[-2]

        if decimals != [0,0]:
            raise ValueError("The decimal number is too large for the custom base system.")
        return tuple(result)
    
    def group_action_2_decimal_action(self,inputs):
        """ 
        Function: Â∞Ü‰∏Ä‰∏™ÂÖ∑Êúâ‰∏çÂêåÂü∫Êï∞ÁöÑÊï∞Â≠óÁ≥ªÁªü(ÊØè‰ΩçÁöÑÂü∫Êï∞ÂàÜÂà´‰∏∫ [8, 8, 8, 6, 5])ËΩ¨Êç¢‰∏∫ÂçÅËøõÂà∂Êï¥Êï∞, ÈúÄË¶ÅÁºñÂÜô‰∏Ä‰∏™PythonÂáΩÊï∞Êù•ÊâßË°åÈÄÜÂêëËÆ°ÁÆó„ÄÇËøô‰∏™ËΩ¨Êç¢Ê∂âÂèäÂ∞ÜÊØè‰ΩçÁöÑÂÄº‰πò‰ª•ÂØπÂ∫îÁöÑÂü∫Êï∞ÁöÑÂπÇ, ÁÑ∂ÂêéÂÜçÊ±ÇÂíå„ÄÇ
        :output: int, ÂçÅËøõÂà∂Êï¥Êï∞
        :number_tuple: tuple, ÊØè‰ΩçÁöÑÂÄº
        :bases: list, ÊØè‰ΩçÁöÑÂü∫Êï∞
        """
        # Á°Æ‰øùËæìÂÖ•ÁöÑÈïøÂ∫¶‰∏éÂü∫Êï∞ÂåπÈÖç
        if len(inputs) != len(self.bases):
            raise ValueError("The input number does not match the expected number of digits.")
        # ÂàùÂßãÂåñÂçÅËøõÂà∂ÁªìÊûú
        decimal_results = [0,0]
        # ËÆ°ÁÆóÂçÅËøõÂà∂ÂÄº
        mid = len(inputs)-2
        for i, digit in enumerate(inputs):
            if digit >= self.bases[i]:
                raise ValueError(f"Digit at position {i} exceeds the base limit of {self.bases[i]-1}.")
            if i < mid:
                decimal_results[0] = decimal_results[0] * self.bases[i] + digit
            else:
                decimal_results[1] = decimal_results[1] * self.bases[i] + digit
        return tuple(decimal_results)
    
    def null_token(self):
        return self.encode_action(self.NULL_ACTION)

    def get_clean_dataset(dataset):
        """
        ÂéªÊéâÁ©∫Âä®‰ΩúÔºåÁî®‰∫éBPEËÆ≠ÁªÉ
        """
        minerl_action,infos= dataset["actions"],dataset["infos"]
        beg_idx,end_idx = 0,len(dataset["actions"]["attack"])
        minerl_action_transformed = {key: np.array(val) for key, val in minerl_action.items()} #‰ªélistÂà∞ndarray
        minerl_action = action_transformer.env2policy(minerl_action_transformed) 
        actions = action_mapper.from_factored(minerl_action,if_inventory=False)
        action_list = split_action(actions) 
        flatten_infos = flatten_dict(infos)     
        
        clean_dataset = [] #[(idx,control_token,image_path)]
        for idx in range(beg_idx,end_idx):
            # get action token
            action = action_list[idx]
            
            control_token = action_tokenizer.encode(action)
            
            # filter the meaningless action
            if control_token==action_tokenizer.null_token() or action_filter(flatten_infos[idx]):
                continue

class BPEActionTokenizer(ActionTokenizer):

    SAVE_CHECKPOINT_INTERVAL = 1000
    LOG_INTERVAL = 20
    
    
    def __init__(self, checkpoint_path= Path("ultron/model/inference/checkpoints/bpe_model_512.pkl"), 
                 train=False,
                 documents_dir =None, 
                 vocab_size: int=512,
                 tokenizer_type="llama-2",
                 camera_quantization_scheme="mu_law",
                 camera_mu=10,
                 n_camera_bins=11,
                 camera_binsize=2
                ):
        
        
        super().__init__(tokenizer_type=tokenizer_type,
            camera_quantization_scheme=camera_quantization_scheme,
            camera_mu=camera_mu,
            n_camera_bins=n_camera_bins,
            camera_binsize=camera_binsize)
        
        self.checkpoint_path=checkpoint_path
        self.camera_null_number = (self.action_transformer.discretize_camera(np.array([0.])) * (n_camera_bins + 1)).item()
        
        self.initial_vocab_size = 2 ** (len(self.movements)) + 9 + 2 ** len(self.operations) + 1 + n_camera_bins ** 2
        self.vocab_size = vocab_size
        
        self.documents = []
        self.total_len = 0      # length of all documents
        
        if train:
            self.initialize_tokenizer(documents_dir)
        else:
            self.load_from_checkpoint()

    def load_from_checkpoint(self,):
        with open(self.checkpoint_path, 'rb') as f:
            checkpoint = pickle.load(f)
            
            self.total_len = checkpoint["total_len"]
            self.idx = checkpoint["idx"]
            self.bpe_rules = checkpoint["bpe_rules"]
            self.reverse_bpe_rules = checkpoint["reverse_bpe_rules"]
            self.frequency = checkpoint["frequency"]
            self.log = checkpoint["log"]
    
    def save_to_checkpoint(self,):
        checkpoint = {
            'total_len': self.total_len,
            'idx': self.idx,
            'bpe_rules': self.bpe_rules,
            'reverse_bpe_rules': self.reverse_bpe_rules,
            'frequency': self.frequency,
            'log': self.log
        }
        with open(self.checkpoint_path, 'wb') as f:
            pickle.dump(checkpoint, f)
        print(f"Checkpoint saved at {self.checkpoint_path}")
    
    def initialize_tokenizer(self, documents_dir):
        self.documents = []
        self.total_len = 0
        self.initial_vocab_size = 2 ** (len(self.movements)) + 9 + 2 ** len(self.operations) + 1 + self.n_camera_bins ** 2
        self.idx = copy.copy(self.initial_vocab_size)
        self.bpe_rules = []
        self.reverse_bpe_rules = {}
        for i in range(self.initial_vocab_size):
            self.reverse_bpe_rules[i] = (i,)
        self.frequency = {}
        
        for document in tqdm(documents_dir.rglob('*.jsonl'), desc="Processing documents"):
            my_document = []
            with open(document, 'rb') as f:
                for line in f:
                    actions = json.loads(line)['actions']
                    traj_len = len(actions['attack'])
                    for i in range(traj_len):
                        action = {key: val[i] for key, val in actions.items()}
                        my_document.extend(self.action2int(action))
            self.documents.append(my_document)
            self.total_len += len(my_document)
        
        for document in tqdm(self.documents, desc="Calculating frequency"):
            for i in range(len(document) - 1):
                pair = (document[i], document[i+1])
                self.frequency[pair] = self.frequency.get(pair, 0) + 1

        self.log = {'vocab_size': [self.initial_vocab_size], 'total_len': [self.total_len]}
    
    def merge(self, pair: tuple, new_token: int):
        new_documents = []
        for document in self.documents:
            new_document = []
            i = 0
            while i < len(document):
                if i < len(document) - 1 and (document[i], document[i+1]) == pair:
                    new_document.append(new_token)
                    self.total_len -= 1
                    # Update frequency
                    if i > 0:
                        self.frequency[(document[i-1], new_token)] = self.frequency.get((document[i-1], new_token), 0) + 1
                    if i < len(document) - 2:
                        self.frequency[(new_token, document[i+2])] = self.frequency.get((new_token, document[i+2]), 0) + 1
                    i += 2
                else:
                    new_document.append(document[i])
                    i += 1
            new_documents.append(new_document)
        self.frequency.pop(pair)
        self.documents = new_documents

    def train(self):
        print(f"Initial vocab size: {self.initial_vocab_size}")
        with tqdm(total=self.vocab_size - self.idx, desc="Training BPE") as pbar:
            while self.idx < self.vocab_size:
                most_frequent_pair = max(self.frequency, key=self.frequency.get)
                self.bpe_rules.append((most_frequent_pair, self.idx))
                self.reverse_bpe_rules[self.idx] = self.reverse_bpe_rules[most_frequent_pair[0]] + self.reverse_bpe_rules[most_frequent_pair[1]]
                self.merge(most_frequent_pair, self.idx)
                self.idx += 1
                pbar.update(1)
                if self.idx % self.SAVE_CHECKPOINT_INTERVAL == 0:
                    with open(checkpoints_dir / f'bpe_checkpoint_{self.idx}.pkl', 'wb') as f:
                        pickle.dump(self, f)
                if self.idx % self.LOG_INTERVAL == 0:
                    self.log['vocab_size'].append(self.idx)
                    self.log['total_len'].append(self.total_len)
        self.save_to_checkpoint()
        print(f"Compress rate is {self.total_len / self.log['total_len'][0]}")

    def encode(self,trajectory:list) -> list[tuple[int]]:
        """Â∞ÜtrajectoryÊò†Â∞ÑÂà∞tokens
        ËæìÂÖ• trajectoryÔºåÂøÖÈ°ªÂåÖÂê´actionsÔºåimage_pathÔºåÂíåËøô‰∏ÄÊ≠•ÁöÑid
        ËæìÂá∫ ÂåÖÂê´action_tokensÔºåimage_paths,Ëµ∑ÂßãÊ≠•ÁöÑid
        """
        actions = trajectory['actions']
        traj_len = len(actions['attack'])
        observations = trajectory.get('observations',[""]*traj_len)
        uuids = trajectory.get('uuids',[""]*traj_len)
        action_ids = self.actions2ints(actions)
        encoded_trajectory = []
        
        last_action_id = 0
        last_action_start_index = 0
        current_index = 0
        while current_index < len(action_ids):
            _, action_id = action_ids[current_index]
            if last_action_id != action_id:
                control_token = self.action_ids2token([action_ids[idx][0] for idx in range(last_action_start_index, current_index)])
                encoded_trajectory.append([control_token, observations[last_action_start_index:current_index], uuids[last_action_start_index]])
                last_action_id = action_id
                last_action_start_index = current_index
            current_index += 1
        
        control_token = self.action_ids2token([action_ids[idx][0] for idx in range(last_action_start_index,  len(action_ids))])
        encoded_trajectory.append([control_token, observations[last_action_start_index:len(action_ids)], uuids[last_action_start_index]])
        return encoded_trajectory
    
    def decode(self, tokens: Union[torch.Tensor,list]) -> list[dict]:
        '''
        decode a chunk of action token to a list of actions
        '''
        action_ids = self.token2action_idx(tokens)
        actions = []
        for action_idx in action_ids:
            if action_idx==-1:
                continue
            actions.extend([self.int2action(t) for t in self.reverse_bpe_rules[action_idx]])
        new_actions = []
        for action in actions:
            minerl_action_transformed = {key: np.array([val]) for key, val in action.items()}
            minerl_action = self.action_transformer.env2policy(minerl_action_transformed) 
            new_action = self.action_mapper.from_factored(minerl_action)
            new_action = {key: val[0] for key, val in new_action.items()}
            new_actions.append(new_action)
        return new_actions 

    def actions2ints(self,actions:dict)-> tuple[int]:
        traj_len = len(actions['attack'])
        old_document = []
        for i in range(traj_len):
            action = {key: val[i] for key, val in actions.items()}
            result = self.action2int(action)
            old_document.extend((token, i) for token in result)
        
        for pair, new_token in self.bpe_rules:
            new_document = []
            i = 0
            while i < len(old_document):
                if i < len(old_document) - 1 and (old_document[i][0], old_document[i+1][0]) == pair:
                    new_document.append((new_token, old_document[i][1]))
                    i += 2
                else:
                    new_document.append(old_document[i])
                    i += 1
            old_document = new_document
        return old_document

    def action2int(self,action: dict)-> tuple[int]:
        result = []
        mov_num = 0
        for i, mov in enumerate(self.movements):
            if action[mov]:
                mov_num += 2 ** i
        if mov_num > 0:
            result.append(mov_num)

        hotbar_num = 0
        for i in range(9):
            if action[f'hotbar.{i+1}']:
                hotbar_num = i
                break
        if hotbar_num > 0:
            result.append(hotbar_num + 2 ** len(self.movements))

        op_num = 0
        for i, op in enumerate(self.operations):
            if action[op]:
                op_num += 2 ** i
        if op_num > 0:
            result.append(op_num + 2 ** (len(self.movements)) + 9)

        if action['inventory']:
            result.append(2 ** (len(self.movements)) + 9 + 2 ** len(self.operations))

        camera_x = self.action_transformer.discretize_camera(np.array([action['camera'][0]]))
        camera_y = self.action_transformer.discretize_camera(np.array([action['camera'][1]]))
        camera_num = (camera_x * self.n_camera_bins + camera_y).item()
        if camera_num != self.camera_null_number:
            result.append(camera_num + 2 ** (len(self.movements)) + 9 + 2 ** len(self.operations) + 1)

        return tuple(result)
        
    def null_token(self):
        return self.action_ids2token([0])

    def int2action(self,token: int) -> dict:
        action = {mov: False for mov in self.movements}
        action.update({f'hotbar.{i+1}': False for i in range(9)})
        action.update({op: False for op in self.operations})
        action['inventory'] = False
        action['camera'] = (0., 0.)

        if token < 2 ** len(self.movements):
            for i, mov in enumerate(self.movements):
                if token & 2 ** i:
                    action[mov] = True
        elif token < 2 ** (len(self.movements)) + 9:
            hotbar_num = token - 2 ** len(self.movements)
            action[f'hotbar.{hotbar_num + 1}'] = True
        elif token < 2 ** (len(self.movements)) + 9 + 2 ** len(self.operations):
            op_num = token - 2 ** (len(self.movements)) - 9
            for i, op in enumerate(self.operations):
                if op_num & 2 ** i:
                    action[op] = True
        elif token < 2 ** (len(self.movements)) + 9 + 2 ** len(self.operations) + 1:
            action['inventory'] = True
        elif token < 2 ** (len(self.movements)) + 9 + 2 ** len(self.operations) + 1 + self.n_camera_bins ** 2:
            camera_num = token - 2 ** (len(self.movements)) - 9 - 2 ** len(self.operations) - 1
            camera_x = camera_num // self.n_camera_bins
            camera_y = camera_num % self.n_camera_bins
            action['camera'] = (self.action_transformer.undiscretize_camera(camera_x).item(), self.action_transformer.undiscretize_camera(camera_y).item())
        else:
            raise ValueError(f"Invalid token {token}")
        return action

    def action_ids2token(self,action_ids:list)->str:
        zero_include_token_list = [map_control_token( action_idx, 0,self.tokenizer_type) for action_idx in action_ids]
        control_token = "".join((s for s in zero_include_token_list))  #cameraÂøÖÈ°ª‰øùÂ≠ò
        tag_control_token = self.act_beg_token + control_token + self.act_end_token
        return tag_control_token
    
    def token2action_idx(self,tokens:Union[torch.Tensor,list]):
        if isinstance(tokens, torch.Tensor):
            # Â¶ÇÊûúÊòØ‰∫åÁª¥Âº†Èáè (shape == 2)ÔºåÂàôÈúÄË¶Å squeeze
            if tokens.ndim == 2:
                tokens = tokens.squeeze()
            tokens = tokens.tolist()
        elif not isinstance(tokens, list):
            raise ValueError("wrong type!")
        action_ids = []
        try:
            first_index_n1 = tokens.index(self.act_beg_id, 0)
            first_index_n2 = tokens.index(self.act_end_id, first_index_n1 + 1)
            control_tokens = tokens[first_index_n1 + 1:first_index_n2]
            for token in control_tokens:
                print(token)
                _, num = remap_control_token(token, use_num=True, tokenizer_type=self.tokenizer_type)
                action_ids.append(num)
            return action_ids
        except ValueError:
            return [0]
         

def make_action_tokenizer(name:str):
    checkpoints_dir = Path('checkpoints')
    checkpoints_dir.mkdir(exist_ok=True)
    with open(checkpoints_dir / name, 'rb') as f:
         loaded_bpe = pickle.load(f)
    return loaded_bpe

if __name__ == '__main__':
    #print(prepare_for_remap_control_token(bases=[512,3,3,3,2,2,2,2,2,11,11]))
    #import pdb; pdb.set_trace()
    #exit()
    #documents_dir = Path('/public/share/craft-shell_agent-12_05')
    #heckpoints_dir = Path('checkpoints')
    #checkpoints_dir.mkdir(exist_ok=True)

    #vocab_size = 512
    #bpe = BPEActionTokenizer(documents_dir=documents_dir, vocab_size=vocab_size,train=True,checkpoint_path=checkpoints_dir / 'bpe_model_512.pkl',)
    #bpe.train()

    #Save the BPE model
    #with open(checkpoints_dir / f'bpe_model_{vocab_size}.pkl', 'wb') as f:
        #pickle.dump(bpe, f)

    # # plot log info
    # plt.plot(bpe.log['vocab_size'], bpe.log['total_len'])
    # plt.xlabel('Vocabulary size')
    # plt.ylabel('Total length of documents')
    # plt.title('BPE training process')
    # plt.savefig('bpe_training.png')

    # Load the BPE model and encode a file
    loaded_bpe = BPEActionTokenizer()

    A = {
        "actions":{'forward': [True], 'back': [False,], 'left': [False,], 'right': [True,], 'sprint': [False,], 'sneak': [False,], 'hotbar.1': [True,], 'hotbar.2': [False,], 'hotbar.3': [False,], 'hotbar.4': [False,], 'hotbar.5': [False,], 'hotbar.6': [False,], 'hotbar.7': [False,], 'hotbar.8': [False,], 'hotbar.9': [False,], 'use': [False,], 'drop': [True,], 'attack': [False,], 'jump': [False,], 'inventory': [False,], 'camera': [(1, 0.0)]}
    }
    #print(loaded_bpe.encode(A))#utils.load_jsonl(documents_dir / 'craft_craft_table/train/009b0681-9120-4eec-b54b-1e2f23f30885.jsonl')[0]))
    print(loaded_bpe.decode([1,29871,31533,30705, 31527,13,376, 29913,31472,31197,]))

    #action_map = ActionTokenizer("llama-2")
    
    #print(action_map.decode(tokens=torch.tensor([128183,128202,128219,128240,128184])))
    #print(action_map.encode([0]))
    
    