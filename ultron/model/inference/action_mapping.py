import re
import numpy as np
from collections import OrderedDict
from typing import Union
import torch
from jarvis.arm.utils.vpt_lib.actions import  ActionTransformer,Buttons
from jarvis.arm.utils.vpt_lib.action_mapping import CameraHierarchicalMapping


def get_special_token(model_id:str = '/nfs-shared/models/llama-3', bases:list = [10,3,3,3,2,2,2,2,2,11,11]) -> list:  #ÂÅáËÆæÊ∞∏Ëøú‰∏ç‰ºöÂá∫Áé∞8641Ëøô‰∏™Êï∞
    '''
    bases: button+camera
    :output: list, ËøîÂõû‰∏Ä‰∏™ÂåÖÂê´ÊâÄÊúâÊú™Áü•tokenÁöÑÂàóË°®
    Function: ÁîüÊàê‰∏Ä‰∏™ÂåÖÂê´ÊâÄÊúâÊú™Áü•tokenÁöÑÂàóË°®, Áî®‰∫éÊ†áËÆ∞Êú™Áü•ÁöÑtoken
    Examples:
    '''
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    token_num = sum(bases)+10
    special_tokens = sorted(list(tokenizer.vocab.items()), key=lambda x: x[-1])[-token_num:]
    return special_tokens

'''
Llama-2, Vicuna-v1.5 
[
    ('ÏßÑ', 31536),('‡¶ú', 31537),('Ï≤ú', 31563),('ÎÖÑ', 31571),('ÏÑ∏', 31578),('ÎØº', 31582),('‡µº', 31585),('·º°', 31598),('Ìò∏', 31603),('‡®∞', 31604),
    ('Í∑∏', 31607),('‡∂±', 31609),('‡Ωì', 31614),
    ('„ÇÜ', 31621),('„Åî', 31622),('ÌòÑ', 31680), 
    ('Íµ∞', 31699), ('Î¨¥', 31716), ('ÏúÑ', 31724), 
    ('Ïïà', 31734), ('Î∞ï', 31736),
    ('Ïö©', 31737), ('Îã®', 31746), 
    ('Î©¥', 31747), ('ÎÇ®', 31754), 
    ('Í∞ï', 31774), ('Ïî®', 31781), 
    ('Í∞ú', 31789), ('Îì§', 31804), 
    ('Ï∞®', 31817), ('Ìïô', 31822), ('Îßå', 31826), ('ÌÑ∞', 31856), ('Ïãù', 31895), ('Í≥º', 31906), ('ÌÉÄ', 31925), ('Ï¢Ö', 31930), ('ÎÇ¥', 31940), ('Ï§ë', 31941), ('Î∞©', 31945), 
    ('Ïõî', 31950), ('Ìöå', 31953), ('Î™®', 31962), ('Î∞î', 31963), ('Ïùå', 31966), ('Ïû¨', 31973), ('Î™Ö', 31976), ('Ìï©', 31980), ('Ïó≠', 31987), ('Î∞±', 31989), ('Ïôï', 31996), 
]
mistral (llava-v1.6-mistral-7b-hf)
[
    ('Êú±', 31947),('«ù', 31948),('·∏®', 31949),('ÊãÖ', 31950),('ÁÅ∞', 31951), ('ËÆ≤', 31952), ('Î°§', 31953),('üò§', 31955),('·üÑ', 31956),('Ïï†', 31957),
    ('ÏòÄ', 31958),('Ïßà', 31959),('ÊåØ', 31960),
    ('ÁÅØ', 31961),('ƒâ', 31962),('‡∑É', 31963),
    ('Èñâ', 31964),('Îû®', 31965),('‡≤Ç', 31966),
    ('„Åí', 31967),('Ãß', 31968),
    ('ÁãÇ', 31969),('Ëûç', 31970),
    ('‰ªç', 31971),('ÂØ¶', 31972),
    ('Ê•Ω', 31973),('ÁØÑ', 31974),
    ('‡∞µ', 31976),('Âµå', 31977),
    ('Êë©', 31978),('Ë¢Å', 31979),('‡¶∑', 31980),('‰πé', 31981),('Í∑ú', 31982),('Â≤ó', 31983),('Á≥ä', 31984),('‡∞ï', 31985),('Èõ≤', 31986),('Ïã¨', 31987),('‡§à', 31988),
    ('‡Ω†', 31989),('·º°', 31990),('‰∏ù', 31991),('ƒ¶', 31992),('Ÿç', 31993),('Ÿì', 31994),('·Ä°', 31995),('Âü∑', 31996),('Î≤®', 31997),('„Çº', 31998),('Ê¢¶', 31999),
]
llama-3,llava-next(llama3-llava-next-8b-hf)
[
    ('<|reserved_special_token_200|>', 128205),('<|reserved_special_token_201|>', 128206),('<|reserved_special_token_202|>', 128207),('<|reserved_special_token_203|>', 128208),('<|reserved_special_token_204|>', 128209),('<|reserved_special_token_205|>', 128210),('<|reserved_special_token_206|>', 128211),('<|reserved_special_token_207|>', 128212),('<|reserved_special_token_208|>', 128213),('<|reserved_special_token_209|>', 128214),
    ('<|reserved_special_token_210|>', 128215),('<|reserved_special_token_211|>', 128216),('<|reserved_special_token_212|>', 128217),
    ('<|reserved_special_token_213|>', 128218),('<|reserved_special_token_214|>', 128219),('<|reserved_special_token_215|>', 128220),
    ('<|reserved_special_token_216|>', 128221),('<|reserved_special_token_217|>', 128222),('<|reserved_special_token_218|>', 128223),
    ('<|reserved_special_token_219|>', 128224),('<|reserved_special_token_220|>', 128225),
    ('<|reserved_special_token_221|>', 128226),('<|reserved_special_token_222|>', 128227),
    ('<|reserved_special_token_223|>', 128228),('<|reserved_special_token_224|>', 128229),
    ('<|reserved_special_token_225|>', 128230),('<|reserved_special_token_226|>', 128231),
    ('<|reserved_special_token_227|>', 128232),('<|reserved_special_token_228|>', 128233),
    ('<|reserved_special_token_229|>', 128234),('<|reserved_special_token_230|>', 128235),('<|reserved_special_token_231|>', 128236),('<|reserved_special_token_232|>', 128237),('<|reserved_special_token_233|>', 128238),('<|reserved_special_token_234|>', 128239),('<|reserved_special_token_235|>', 128240),('<|reserved_special_token_236|>', 128241),('<|reserved_special_token_237|>', 128242),('<|reserved_special_token_238|>', 128243),('<|reserved_special_token_239|>', 128244),
    ('<|reserved_special_token_240|>', 128245),('<|reserved_special_token_241|>', 128246),('<|reserved_special_token_242|>', 128247),('<|reserved_special_token_243|>', 128248), ('<|reserved_special_token_244|>', 128249),('<|reserved_special_token_245|>', 128250),('<|reserved_special_token_246|>', 128251),('<|reserved_special_token_247|>', 128252),('<|reserved_special_token_248|>', 128253),('<|reserved_special_token_249|>', 128254),('<|reserved_special_token_250|>', 128255),
]
Fuyu
[
    ('s‚ñÅflu', 262109), ('all‚ñÅour', 262110), ('og‚ñÅi', 262111), ('t‚ñÅtri', 262112), ('ank‚ñÅyou', 262113),('nia‚ñÅi', 262114), ('le‚ñÅtr', 262115), ('s‚ñÅdoing‚ñÅthe', 262116),
    ('makes‚ñÅthe', 262117), ('createT', 262118), ('refectur', 262119), ('aditional', 262120), ('staken', 262121), ('‚ñÅa‚ñÅMar', 262122), ('‚ñÅthe‚ñÅDis', 262123), ('‚ñÅother‚ñÅdev', 262124), 
    ('servato', 262125), ('others‚ñÅwho', 262126), ('de‚ñÅpos', 262127), ('esirable', 262128), ('sign‚ñÅan', 262129), ('age‚ñÅrating', 262130), ('lementation', 262131), ('wondering‚ñÅwhat', 262132),
    ('st‚ñÅat‚ñÅthe', 262133), ('which‚ñÅis‚ñÅan', 262134), ('in‚ñÅlight‚ñÅof‚ñÅthe', 262135), ('plied‚ñÅby', 262136),('se‚ñÅdet', 262137), ('avies', 262138),
    ('of‚ñÅpro', 262139), ('‚ñÅthe‚ñÅpheno', 262140), ('This‚ñÅstudy‚ñÅwas', 262141), ('ion‚ñÅtemperature', 262142), ('cause‚ñÅeveryone', 262143)
]
'''

def map_control_token(num:int, place:int, tokenizer_type:str = "llama-2",not_text=False) -> str:
    if tokenizer_type == "llama-2":
        special_tokens = [
            (('ÏßÑ', 31536),('‡¶ú', 31537),('Ï≤ú', 31563),('ÎÖÑ', 31571),('ÏÑ∏', 31578),('ÎØº', 31582),('‡µº', 31585),('·º°', 31598),('Ìò∏', 31603),('‡®∞', 31604),),
            (('Í∑∏', 31607),('‡∂±', 31609),('‡Ωì', 31614),),
            (('„ÇÜ', 31621),('„Åî', 31622),('ÌòÑ', 31680),),
            (('Íµ∞', 31699), ('Î¨¥', 31716), ('ÏúÑ', 31724),),
            (('Ïïà', 31734), ('Î∞ï', 31736),),
            (('Ïö©', 31737), ('Îã®', 31746),),
            (('Î©¥', 31747), ('ÎÇ®', 31754),),
            (('Í∞ï', 31774), ('Ïî®', 31781),),
            #(('Í∞ú', 31789), ('Îì§', 31804),),
            (('Ï∞®', 31817), ('Ìïô', 31822), ('Îßå', 31826), ('ÌÑ∞', 31856), ('Ïãù', 31895), ('Í≥º', 31906), ('ÌÉÄ', 31925), ('Ï¢Ö', 31930), ('ÎÇ¥', 31940), ('Ï§ë', 31941), ('Î∞©', 31945)),
            (('Ïõî', 31950), ('Ìöå', 31953), ('Î™®', 31962), ('Î∞î', 31963), ('Ïùå', 31966), ('Ïû¨', 31973), ('Î™Ö', 31976), ('Ìï©', 31980), ('Ïó≠', 31987), ('Î∞±', 31989), ('Ïôï', 31996)),
        ]
    elif tokenizer_type == "mistral":
        special_tokens = [
            (('Êú±', 31947),('«ù', 31948),('·∏®', 31949),('ÊãÖ', 31950),('ÁÅ∞', 31951), ('ËÆ≤', 31952), ('Î°§', 31953),('üò§', 31955),('·üÑ', 31956),('Ïï†', 31957),),
            (('ÏòÄ', 31958),('Ïßà', 31959),('ÊåØ', 31960),),
            (('ÁÅØ', 31961),('ƒâ', 31962),('‡∑É', 31963),),
            (('Èñâ', 31964),('Îû®', 31965),('‡≤Ç', 31966),),
            (('„Åí', 31967),('„Åµ', 31896),),
            (('ÁãÇ', 31969),('Ëûç', 31970),),
            (('‰ªç', 31971),('ÂØ¶', 31972),),
            (('Ê•Ω', 31973),('ÁØÑ', 31974),),
            #(('‡∞µ', 31976),('Âµå', 31977),),
            (('Êë©', 31978),('Ë¢Å', 31979),('‡¶∑', 31980),('‰πé', 31981),('Í∑ú', 31982),('Â≤ó', 31983),('Á≥ä', 31984),('‡∞ï', 31985),('Èõ≤', 31986),('Ïã¨', 31987),('‡§à', 31988),),
            (('‡Ω†', 31989),('·º°', 31990),('‰∏ù', 31991),('ƒ¶', 31992),('‰ºù', 31993),('Ïª®', 31885),('·Ä°', 31995),('Âü∑', 31996),('Î≤®', 31997),('„Çº', 31998),('Ê¢¶', 31999),),
        ]
    elif tokenizer_type == "llama-3":
        special_tokens = [
            (('<|reserved_special_token_200|>', 128205),('<|reserved_special_token_201|>', 128206),('<|reserved_special_token_202|>', 128207),('<|reserved_special_token_203|>', 128208),('<|reserved_special_token_204|>', 128209),('<|reserved_special_token_205|>', 128210),('<|reserved_special_token_206|>', 128211),('<|reserved_special_token_207|>', 128212),('<|reserved_special_token_208|>', 128213),('<|reserved_special_token_209|>', 128214),),
            (('<|reserved_special_token_210|>', 128215),('<|reserved_special_token_211|>', 128216),('<|reserved_special_token_212|>', 128217),),
            (('<|reserved_special_token_213|>', 128218),('<|reserved_special_token_214|>', 128219),('<|reserved_special_token_215|>', 128220),),
            (('<|reserved_special_token_216|>', 128221),('<|reserved_special_token_217|>', 128222),('<|reserved_special_token_218|>', 128223),),
            (('<|reserved_special_token_219|>', 128224),('<|reserved_special_token_220|>', 128225),),
            (('<|reserved_special_token_221|>', 128226),('<|reserved_special_token_222|>', 128227),),
            (('<|reserved_special_token_223|>', 128228),('<|reserved_special_token_224|>', 128229),),
            (('<|reserved_special_token_225|>', 128230),('<|reserved_special_token_226|>', 128231),),
            #(('<|reserved_special_token_227|>', 128232),('<|reserved_special_token_228|>', 128233),),
            (('<|reserved_special_token_229|>', 128234),('<|reserved_special_token_230|>', 128235),('<|reserved_special_token_231|>', 128236),('<|reserved_special_token_232|>', 128237),('<|reserved_special_token_233|>', 128238),('<|reserved_special_token_234|>', 128239),('<|reserved_special_token_235|>', 128240),('<|reserved_special_token_236|>', 128241),('<|reserved_special_token_237|>', 128242),('<|reserved_special_token_238|>', 128243),('<|reserved_special_token_239|>', 128244),),
            (('<|reserved_special_token_240|>', 128245),('<|reserved_special_token_241|>', 128246),('<|reserved_special_token_242|>', 128247),('<|reserved_special_token_243|>', 128248), ('<|reserved_special_token_244|>', 128249),('<|reserved_special_token_245|>', 128250),('<|reserved_special_token_246|>', 128251),('<|reserved_special_token_247|>', 128252),('<|reserved_special_token_248|>', 128253),('<|reserved_special_token_249|>', 128254),('<|reserved_special_token_250|>', 128255),),
        ]
    else:
        raise ValueError(f"The tokenizer type {tokenizer_type} is not supported in control tokens.")
    return special_tokens[place][num][not_text]

def prepare_for_remap_control_token(tokenizer_type:str = "llama-2",bases:list = [10,3,3,3,2,2,2,2,2,11,11]):
    
    tokens = {}
    for i,base in enumerate(bases):
        for j in range(base):
            token = map_control_token(j,i,tokenizer_type)
            tokens[token]=(i,j)
    return tokens

def remap_control_token(token:str,use_num=True, tokenizer_type:str = "llama-2")->tuple:
    """Áî±tokenÊò†Â∞ÑÂà∞actionÔºåÊ≥®ÊÑèÔºåËôΩÁÑ∂Êääcamera‰ªétoken‰∏≠ÂéªÊéâÔºå‰ΩÜÊòØËøòÈúÄË¶ÅÂÆÉ """
    re_tokens = {}
    if tokenizer_type == "llama-2":
        if use_num:
            re_tokens = {31536:(0, 0), 31537:(0, 1), 31563:(0, 2), 31571:(0, 3), 31578:(0, 4), 31582:(0, 5), 31585:(0, 6), 31598:(0, 7), 31603:(0, 8), 31604:(0, 9), 31607:(1, 0), 31609:(1, 1), 31614:(1, 2), 31621:(2, 0), 31622:(2, 1), 31680:(2, 2), 31699:(3, 0), 31716:(3, 1), 31724:(3, 2), 31734:(4, 0), 31736:(4, 1), 31737:(5, 0), 31746:(5, 1), 31747:(6, 0), 31754:(6, 1), 31774:(7, 0), 31781:(7, 1), 31789:(8, 0), 31804:(8, 1), 31817:(9, 0), 31822:(9, 1), 31826:(9, 2), 31856:(9, 3), 31895:(9, 4), 31906:(9, 5), 31925:(9, 6), 31930:(9, 7), 31940:(9, 8), 31941:(9, 9), 31945:(9, 10), 31950:(10, 0), 31953:(10, 1), 31962:(10, 2), 31963:(10, 3), 31966:(10, 4), 31973:(10, 5), 31976:(10, 6), 31980:(10, 7), 31987:(10, 8), 31989:(10, 9), 31996:(10, 10)}
        else:
            re_tokens = {
                'ÏßÑ': (0, 0),'‡¶ú': (0, 1),'Ï≤ú': (0, 2),'ÎÖÑ': (0, 3),'ÏÑ∏': (0, 4),'ÎØº': (0, 5),'‡µº': (0, 6),'·º°': (0, 7),'Ìò∏': (0, 8),'‡®∞': (0, 9),
                'Í∑∏': (1, 0),'‡∂±': (1, 1),'‡Ωì': (1, 2),
                '„ÇÜ': (2, 0),'„Åî': (2, 1),'ÌòÑ': (2, 2),
                'Íµ∞': (3, 0),'Î¨¥': (3, 1),'ÏúÑ': (3, 2),
                'Ïïà': (4, 0),'Î∞ï': (4, 1),
                'Ïö©': (5, 0),'Îã®': (5, 1),
                'Î©¥': (6, 0),'ÎÇ®': (6, 1),
                'Í∞ï': (7, 0),'Ïî®': (7, 1),
                'Í∞ú': (8, 0),'Îì§': (8, 1),
                'Ï∞®': (9, 0),'Ìïô': (9, 1),'Îßå': (9, 2),'ÌÑ∞': (9, 3),'Ïãù': (9, 4),'Í≥º': (9, 5),'ÌÉÄ': (9, 6),'Ï¢Ö': (9, 7),'ÎÇ¥': (9, 8),'Ï§ë': (9, 9),'Î∞©': (9, 10),
                'Ïõî': (10, 0),'Ìöå': (10, 1),'Î™®': (10, 2),'Î∞î': (10, 3),'Ïùå': (10, 4),'Ïû¨': (10, 5),'Î™Ö': (10, 6),'Ìï©': (10, 7),'Ïó≠': (10, 8),'Î∞±': (10, 9),'Ïôï': (10, 10)
            }
    elif tokenizer_type=="mistral":
        if use_num:
            re_tokens = {31947:(0, 0), 31948:(0, 1), 31949:(0, 2), 31950:(0, 3), 31951:(0, 4), 31952:(0, 5), 31953:(0, 6), 31955:(0, 7), 31956:(0, 8), 31957:(0, 9), 31958:(1, 0), 31959:(1, 1), 31960:(1, 2), 31961:(2, 0), 31962:(2, 1), 31963:(2, 2), 31964:(3, 0), 31965:(3, 1), 31966:(3, 2), 31967:(4, 0), 31896:(4, 1), 31969:(5, 0), 31970:(5, 1), 31971:(6, 0), 31972:(6, 1), 31973:(7, 0), 31974:(7, 1), 31976:(8, 0), 31977:(8, 1), 31978:(9, 0), 31979:(9, 1), 31980:(9, 2), 31981:(9, 3), 31982:(9, 4), 31983:(9, 5), 31984:(9, 6), 31985:(9, 7), 31986:(9, 8), 31987:(9, 9), 31988:(9, 10), 31989:(10, 0), 31990:(10, 1), 31991:(10, 2), 31992:(10, 3), 31993:(10, 4), 31885:(10, 5), 31995:(10, 6), 31996:(10, 7), 31997:(10, 8), 31998:(10, 9), 31999:(10, 10)}
        else:
            re_tokens = {
                'Êú±': (0, 0),'«ù': (0, 1),'·∏®': (0, 2),'ÊãÖ': (0, 3),'ÁÅ∞': (0, 4),'ËÆ≤': (0, 5),'Î°§': (0, 6),'üò§': (0, 7),'·üÑ': (0, 8),'Ïï†': (0, 9),
                'ÏòÄ': (1, 0),'Ïßà': (1, 1),'ÊåØ': (1, 2),
                'ÁÅØ': (2, 0),'ƒâ': (2, 1),'‡∑É': (2, 2),
                'Èñâ': (3, 0),'Îû®': (3, 1),'‡≤Ç': (3, 2),
                '„Åí': (4, 0),'„Åµ': (4, 1),
                'ÁãÇ': (5, 0),'Ëûç': (5, 1),
                '‰ªç': (6, 0),'ÂØ¶': (6, 1),
                'Ê•Ω': (7, 0),'ÁØÑ': (7, 1),
                '‡∞µ': (8, 0),'Âµå': (8, 1),
                'Êë©': (9, 0),'Ë¢Å': (9, 1),'‡¶∑': (9, 2),'‰πé': (9, 3),'Í∑ú': (9, 4),'Â≤ó': (9, 5),'Á≥ä': (9, 6),'‡∞ï': (9, 7),'Èõ≤': (9, 8),'Ïã¨': (9, 9),'‡§à': (9, 10),
                '‡Ω†': (10, 0),'·º°': (10, 1),'‰∏ù': (10, 2),'ƒ¶': (10, 3),'‰ºù': (10, 4),'Ïª®': (10, 5),'·Ä°': (10, 6),'Âü∑': (10, 7),'Î≤®': (10, 8),'„Çº': (10, 9),'Ê¢¶': (10, 10)
            }
    elif tokenizer_type=="llama-3":
        if use_num:
            re_tokens={128205:(0, 0), 128206:(0, 1), 128207:(0, 2), 128208:(0, 3), 128209:(0, 4), 128210:(0, 5), 128211:(0, 6), 128212:(0, 7), 128213:(0, 8), 128214:(0, 9), 128215:(1, 0), 128216:(1, 1), 128217:(1, 2), 128218:(2, 0), 128219:(2, 1), 128220:(2, 2), 128221:(3, 0), 128222:(3, 1), 128223:(3, 2), 128224:(4, 0), 128225:(4, 1), 128226:(5, 0), 128227:(5, 1), 128228:(6, 0), 128229:(6, 1), 128230:(7, 0), 128231:(7, 1), 128232:(8, 0), 128233:(8, 1), 128234:(9, 0), 128235:(9, 1), 128236:(9, 2), 128237:(9, 3), 128238:(9, 4), 128239:(9, 5), 128240:(9, 6), 128241:(9, 7), 128242:(9, 8), 128243:(9, 9), 128244:(9, 10), 128245:(10, 0), 128246:(10, 1), 128247:(10, 2), 128248:(10, 3), 128249:(10, 4), 128250:(10, 5), 128251:(10, 6), 128252:(10, 7), 128253:(10, 8), 128254:(10, 9), 128255:(10, 10)}
        else:
            re_tokens = {
                '<|reserved_special_token_200|>': (0, 0),'<|reserved_special_token_201|>': (0, 1),'<|reserved_special_token_202|>': (0, 2),'<|reserved_special_token_203|>': (0, 3),'<|reserved_special_token_204|>': (0, 4),'<|reserved_special_token_205|>': (0, 5),'<|reserved_special_token_206|>': (0, 6),'<|reserved_special_token_207|>': (0, 7),'<|reserved_special_token_208|>': (0, 8),'<|reserved_special_token_209|>': (0, 9),
                '<|reserved_special_token_210|>': (1, 0),'<|reserved_special_token_211|>': (1, 1),'<|reserved_special_token_212|>': (1, 2),
                '<|reserved_special_token_213|>': (2, 0),'<|reserved_special_token_214|>': (2, 1),'<|reserved_special_token_215|>': (2, 2),
                '<|reserved_special_token_216|>': (3, 0),'<|reserved_special_token_217|>': (3, 1),'<|reserved_special_token_218|>': (3, 2),
                '<|reserved_special_token_219|>': (4, 0),'<|reserved_special_token_220|>': (4, 1),
                '<|reserved_special_token_221|>': (5, 0),'<|reserved_special_token_222|>': (5, 1),
                '<|reserved_special_token_223|>': (6, 0),'<|reserved_special_token_224|>': (6, 1),
                '<|reserved_special_token_225|>': (7, 0),'<|reserved_special_token_226|>': (7, 1),
                '<|reserved_special_token_227|>': (8, 0),'<|reserved_special_token_228|>': (8, 1),
                '<|reserved_special_token_229|>': (9, 0),'<|reserved_special_token_230|>': (9, 1),'<|reserved_special_token_231|>': (9, 2),'<|reserved_special_token_232|>': (9, 3),'<|reserved_special_token_233|>': (9, 4),'<|reserved_special_token_234|>': (9, 5),'<|reserved_special_token_235|>': (9, 6),'<|reserved_special_token_236|>': (9, 7),'<|reserved_special_token_237|>': (9, 8),'<|reserved_special_token_238|>': (9, 9),'<|reserved_special_token_239|>': (9, 10),
                '<|reserved_special_token_240|>': (10, 0),'<|reserved_special_token_241|>': (10, 1),'<|reserved_special_token_242|>': (10, 2),'<|reserved_special_token_243|>': (10, 3),'<|reserved_special_token_244|>': (10, 4),'<|reserved_special_token_245|>': (10, 5),'<|reserved_special_token_246|>': (10, 6),'<|reserved_special_token_247|>': (10, 7),'<|reserved_special_token_248|>': (10, 8),'<|reserved_special_token_249|>': (10, 9),'<|reserved_special_token_250|>': (10, 10)
            }
    else:
        raise ValueError(f"The tokenizer type {tokenizer_type} is not supported in control tokens.")
    return re_tokens.get(token,(-1,-1))


def tag_token(place, tokenizer_type:str = "llama-2",return_type:int=0):
    """ÂºïÂÖ•Â§¥Ê†áËÆ∞ÂíåÂ∞æÊ†áËÆ∞ """
    assert place in {0,1}
    if tokenizer_type == "llama-2":
        special_tokens = [('Ïú†', 31533),('Ïöî', 31527)]
    elif tokenizer_type == "mistral":
        special_tokens = [('‡≤Æ', 31941),('·ä†', 31942)]
    elif tokenizer_type=="llama-3":
        special_tokens = [('<|reserved_special_token_199|>', 128204),('<|reserved_special_token_198|>', 128203)]
    else:
        raise ValueError(f"The tokenizer type {tokenizer_type} is not supported in control tokens.")
    return special_tokens[place][return_type]



def token_2_action(tokens:Union[str,torch.Tensor],tag_token_list, tokenizer_type:str = 'llama-2',bases:list = [10,3,3,3,2,2,2,2,2,11,11]) -> tuple:
    """Â∞Ü‰∏Ä‰∏™ËæìÂÖ•Â∫èÂàóËΩ¨Êç¢Âõû """
    actions = [0]*len(bases) #ÂàùÂßãÂåñ
    camera_null = [bases[-1]//2,bases[-2]//2]
    actions[-2:] = camera_null
    if isinstance(tokens,str):
        #ËæìÂÖ•ÊñáÂ≠ó
        pattern = f'{tag_token(0,tokenizer_type)}.*{tag_token(1,tokenizer_type)}'
        match = re.search(pattern, tokens)
        
        if not match:
            return custom_seq_2_decimal(actions)
        control_tokens = match.group()[1:-1]
        for token in control_tokens:
            place,num = remap_control_token(token,use_num=False,tokenizer_type=tokenizer_type)
            if place!=-1:
                actions[place]=num
    elif isinstance(tokens,torch.Tensor):

        indices_n1 = torch.where(tokens == tag_token_list[0])
        
        first_index_n1 = indices_n1[1][0].item() if indices_n1[0].numel() > 0 else None

        indices_n2 = torch.where(tokens == tag_token_list[1])
        first_index_n2 = indices_n2[1][0].item() if indices_n2[0].numel() > 0 else None

        if first_index_n1 is not None and first_index_n2 is not None and first_index_n1 < first_index_n2:
            control_tokens = tokens[0][first_index_n1 + 1:first_index_n2]
        else:
            return custom_seq_2_decimal(actions)
        for token in control_tokens:
            place,num = remap_control_token(token.item(),use_num=True,tokenizer_type=tokenizer_type)
            if place!=-1:
                actions[place]=num
        
    else:
        raise ValueError("wrong type!")
    # Â¶ÇÊûúÁßªÂä®‰∫ÜËßÜÈáéÔºåcamera buttonÂèò‰∏∫1
    if actions[-2:] != camera_null:
        actions[-3] = 1
    outputs = custom_seq_2_decimal(actions)
    return outputs
        
def action_2_token(inputs:tuple, tokenizer_type:str = 'llama-2'):
    '''
    Params: 
    inputs:tuple:‰∏§‰∏™ÂçÅËøõÂà∂Êï∞Â≠ó
    * output: str, ËøîÂõû‰∏Ä‰∏™ÊéßÂà∂token
    Function: ÁîüÊàê‰∏Ä‰∏™ÊéßÂà∂token
    Examples:
    1. generate_control_token(15359) -> tuple(7,7,7,5,4) -> 'Îã®ÌïôÏ§ëÏùåÎ∞±'
    2. generate_control_token(1) -> 'tuple(0,0,0,0,1) -> 'ÌòÑÎ©¥ÎßåÎ∞©Ïû¨'
    '''
    # ÁîüÊàêÊéßÂà∂token
    assert len(inputs)==2
    null_action = (0,60)
    custom_seq = decimal_2_custom_seq(inputs)
    zero_include_token_list = [map_control_token(num, i, tokenizer_type) for i, num in enumerate(custom_seq)]
    control_token = ''.join((s for x,s in zip(custom_seq[:-2],zero_include_token_list[:-2]) if x != 0 ))
    control_token = control_token + "".join((s for s in zero_include_token_list[-2:]))  #cameraÂøÖÈ°ª‰øùÂ≠ò
    tag_control_token = tag_token(0,tokenizer_type) + control_token + tag_token(1,tokenizer_type)
    return tag_control_token,inputs==null_action

def decimal_2_custom_seq(inputs:tuple, bases:list = [10,3,3,3,2,2,2,2,11,11]) -> tuple:
    '''
    Params:
    * output: set, ËøîÂõû‰∏Ä‰∏™ÂÖÉÁªÑ, ÂÖÉÁªÑ‰∏≠ÁöÑÊØè‰∏™ÂÖÉÁ¥†Ë°®Á§∫‰∏Ä‰∏™‰ΩçÁöÑÂÄº
    * inputs: tuple, ‰∏§‰∏™ÂçÅËøõÂà∂Êï¥Êï∞
    * bases: list, ÊØè‰ΩçÁöÑÂü∫Êï∞     

    Function: Â∞Ü‰∏Ä‰∏™ÂçÅËøõÂà∂Êï¥Êï∞ËΩ¨Êç¢‰∏∫ÂÖ∑Êúâ‰∏çÂêåÂü∫Êï∞ÁöÑÊï∞Â≠óÁ≥ªÁªü(ÊØè‰ΩçÁöÑÂü∫Êï∞ÂàÜÂà´‰∏∫ [8, 8, 8, 6, 5]), ÈúÄË¶ÅÁºñÂÜô‰∏Ä‰∏™PythonÂáΩÊï∞Êù•ÊâßË°åÈÄÜÂêëËÆ°ÁÆó„ÄÇËøô‰∏™ËΩ¨Êç¢Ê∂âÂèäÂ∞ÜÂçÅËøõÂà∂Êï∞ÈÄê‰ΩçÈô§‰ª•ÂØπÂ∫îÁöÑÂü∫Êï∞Âπ∂Âèñ‰ΩôÊï∞, ÁÑ∂ÂêéÂÜçÁªßÁª≠Â§ÑÁêÜÂïÜ„ÄÇ
    Examples: 
    1. decimal_to_custom_base(1) -> (0, 0, 0, 0, 1)
    2. decimal_to_custom_base(15359) -> (7, 7, 7, 5, 4)
    '''
    decimals = list(inputs)
    decimals[0] = decimals[0]//2 #cameraÈîÆÂú®ËøôÈáåÊ≤°ÊúâÊÑè‰πâ
    # Áî®‰∫éÂ≠òÂÇ®ËΩ¨Êç¢ÁªìÊûúÁöÑÂàóË°®
    result = [0] * len(bases)
    # ‰ªéÊúÄ‰Ωé‰ΩçÂà∞ÊúÄÈ´ò‰ΩçÈÄê‰ΩçËÆ°ÁÆó
    for i in range(len(bases)-3, -1, -1):
        # Ê±ÇÂΩìÂâç‰ΩçÁöÑÂÄº
        result[i] = decimals[0] % bases[i]
        # Êõ¥Êñ∞ÂçÅËøõÂà∂Êï∞‰∏∫‰∏ã‰∏Ä‰ΩçÁöÑÂ§ÑÁêÜ
        decimals[0] //= bases[i]
    # Á°Æ‰øùËΩ¨Êç¢ËøáÁ®ã‰∏≠ÂçÅËøõÂà∂Êï∞Ë¢´ÂÆåÂÖ®ËΩ¨Êç¢
    result[-1] = decimals[1] % bases[-1]
    decimals[1] //= bases[-1]
    result[-2] = decimals[1] % bases[-2]
    decimals[1] //= bases[-2]

    if decimals != [0,0]:
        raise ValueError("The decimal number is too large for the custom base system.")
    return tuple(result)


def custom_seq_2_decimal(number_tuple:tuple, bases:list = [10,3,3,3,2,2,2,2,2,11,11]) -> tuple:
    '''
    ÂÅáÂ¶Çbases‰∏∫[10,3,3,3,2,2,2,2,2,11,11]
    Function: Â∞Ü‰∏Ä‰∏™ÂÖ∑Êúâ‰∏çÂêåÂü∫Êï∞ÁöÑÊï∞Â≠óÁ≥ªÁªü(ÊØè‰ΩçÁöÑÂü∫Êï∞ÂàÜÂà´‰∏∫ [8, 8, 8, 6, 5])ËΩ¨Êç¢‰∏∫ÂçÅËøõÂà∂Êï¥Êï∞, ÈúÄË¶ÅÁºñÂÜô‰∏Ä‰∏™PythonÂáΩÊï∞Êù•ÊâßË°åÈÄÜÂêëËÆ°ÁÆó„ÄÇËøô‰∏™ËΩ¨Êç¢Ê∂âÂèäÂ∞ÜÊØè‰ΩçÁöÑÂÄº‰πò‰ª•ÂØπÂ∫îÁöÑÂü∫Êï∞ÁöÑÂπÇ, ÁÑ∂ÂêéÂÜçÊ±ÇÂíå„ÄÇ
    Examples:
    1. custom_base_to_decimal((0, 0, 0, 0, 1)) -> 1
    2. custom_base_to_decimal((7, 7, 7, 5, 4)) -> 15359
    :output: int, ÂçÅËøõÂà∂Êï¥Êï∞
    :number_tuple: tuple, ÊØè‰ΩçÁöÑÂÄº
    :bases: list, ÊØè‰ΩçÁöÑÂü∫Êï∞
    '''
    # Á°Æ‰øùËæìÂÖ•ÁöÑÈïøÂ∫¶‰∏éÂü∫Êï∞ÂåπÈÖç
    if len(number_tuple) != len(bases):
        raise ValueError("The input number does not match the expected number of digits.")
    # ÂàùÂßãÂåñÂçÅËøõÂà∂ÁªìÊûú
    decimal_results = [0,0]
    # ËÆ°ÁÆóÂçÅËøõÂà∂ÂÄº
    mid = len(number_tuple)-2
    for i, digit in enumerate(number_tuple):
        if digit >= bases[i]:
            raise ValueError(f"Digit at position {i} exceeds the base limit of {bases[i]-1}.")
        if i < mid:
            decimal_results[0] = decimal_results[0] * bases[i] + digit
        else:
            decimal_results[1] = decimal_results[1] * bases[i] + digit
    return tuple(decimal_results)


class ActionMap:
    def __init__(self,tokenizer_type="llama-2",bases=[10,3,3,3,2,2,2,2,2,11,11]):
        self.tokenizer_type = tokenizer_type
        self.bases = bases
        self.action_transformer = ActionTransformer()
        self.action_mapper = CameraHierarchicalMapping(n_camera_bins=11)
        self.basic_tag_token = [tag_token(0,self.tokenizer_type,return_type=1),tag_token(1,self.tokenizer_type,return_type=1)]
    
    def map(self,tokens):
        action = token_2_action(tokens,tag_token_list=self.basic_tag_token,tokenizer_type=self.tokenizer_type,bases=self.bases)
        action_dict = {
            "buttons":np.array([action[0]]),
            "camera":np.array([action[1]]),
        }
        action_dict = OrderedDict({key: value[0] for key, value in action_dict.items()})
        #factored_action = self.action_mapper.to_factored(action_dict)
        #envir_action = self.action_transformer.policy2env(factored_action)
        return action_dict
    
if __name__ == "__main__":
    #outp = token_2_action(tokens=torch.tensor([128204,128235,128247,128203]),tag_token=[128204,128203],tokenizer_type="llama-3")
    #print(outp)
    #exit()
    print(get_special_token("/nfs-shared/models/llava-v1.6-mistral-7b-hf"))
    #action_map = ActionMap("llama-3")
    #print(action_map.map(tokens=torch.tensor([128204,128237,128247,128203])))