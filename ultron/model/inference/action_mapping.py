import re
import numpy as np
from collections import OrderedDict
from jarvis.arm.utils.vpt_lib.actions import  ActionTransformer,Buttons
from jarvis.arm.utils.vpt_lib.action_mapping import CameraHierarchicalMapping


def get_special_token(model_id:str = '/nfs-shared/models/llama-3', bases:list = [10,3,3,3,2,2,2,2,2,11,11]) -> list:  #å‡è®¾æ°¸è¿œä¸ä¼šå‡ºç°8641è¿™ä¸ªæ•°
    '''
    bases: button+camera
    :output: list, è¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰æœªçŸ¥tokençš„åˆ—è¡¨
    Function: ç”Ÿæˆä¸€ä¸ªåŒ…å«æ‰€æœ‰æœªçŸ¥tokençš„åˆ—è¡¨, ç”¨äºæ ‡è®°æœªçŸ¥çš„token
    Examples:
    '''
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    token_num = sum(bases)+10
    special_tokens = sorted(list(tokenizer.vocab.items()), key=lambda x: x[-1])[-token_num:]
    return special_tokens

'''
Llama-2, Vicuna-v1.5 
[
    ('ì§„', 31536),('à¦œ', 31537),('ì²œ', 31563),('ë…„', 31571),('ì„¸', 31578),('ë¯¼', 31582),('àµ¼', 31585),('á¼¡', 31598),('í˜¸', 31603),('à¨°', 31604),
    ('ê·¸', 31607),('à¶±', 31609),('à½“', 31614),
    ('ã‚†', 31621),('ã”', 31622),('í˜„', 31680), 
    ('êµ°', 31699), ('ë¬´', 31716), ('ìœ„', 31724), 
    ('ì•ˆ', 31734), ('ë°•', 31736),
    ('ìš©', 31737), ('ë‹¨', 31746), 
    ('ë©´', 31747), ('ë‚¨', 31754), 
    ('ê°•', 31774), ('ì”¨', 31781), 
    ('ê°œ', 31789), ('ë“¤', 31804), 
    ('ì°¨', 31817), ('í•™', 31822), ('ë§Œ', 31826), ('í„°', 31856), ('ì‹', 31895), ('ê³¼', 31906), ('íƒ€', 31925), ('ì¢…', 31930), ('ë‚´', 31940), ('ì¤‘', 31941), ('ë°©', 31945), 
    ('ì›”', 31950), ('íšŒ', 31953), ('ëª¨', 31962), ('ë°”', 31963), ('ìŒ', 31966), ('ì¬', 31973), ('ëª…', 31976), ('í•©', 31980), ('ì—­', 31987), ('ë°±', 31989), ('ì™•', 31996), 
]
llava-v1.6 (llava-v1.6-mistral-7b-hf,llava-v1.6-vicuna-13b-hf,llava-v1.6-vicuna-7b-hf )
[
    ('æœ±', 31947),('Ç', 31948),('á¸¨', 31949),('æ‹…', 31950),('ç°', 31951), ('è®²', 31952), ('ë¡¤', 31953),('ğŸ˜¤', 31955),('áŸ„', 31956),('ì• ', 31957),
    ('ì˜€', 31958),('ì§ˆ', 31959),('æŒ¯', 31960),
    ('ç¯', 31961),('Ä‰', 31962),('à·ƒ', 31963),
    ('é–‰', 31964),('ë¨', 31965),('à²‚', 31966),
    ('ã’', 31967),('Ì§', 31968),
    ('ç‹‚', 31969),('è', 31970),
    ('ä»', 31971),('å¯¦', 31972),
    ('æ¥½', 31973),('ç¯„', 31974),
    ('à°µ', 31976),('åµŒ', 31977),
    ('æ‘©', 31978),('è¢', 31979),('à¦·', 31980),('ä¹', 31981),('ê·œ', 31982),('å²—', 31983),('ç³Š', 31984),('à°•', 31985),('é›²', 31986),('ì‹¬', 31987),('à¤ˆ', 31988),
    ('à½ ', 31989),('á¼¡', 31990),('ä¸', 31991),('Ä¦', 31992),('Ù', 31993),('Ù“', 31994),('á€¡', 31995),('åŸ·', 31996),('ë²¨', 31997),('ã‚¼', 31998),('æ¢¦', 31999),
]
llama-3,llava-next(llama3-llava-next-8b-hf)
[
    ('<|reserved_special_token_200|>', 128205),('<|reserved_special_token_201|>', 128206),('<|reserved_special_token_202|>', 128207),('<|reserved_special_token_203|>', 128208),('<|reserved_special_token_204|>', 128209),('<|reserved_special_token_205|>', 128210),('<|reserved_special_token_206|>', 128211),('<|reserved_special_token_207|>', 128212),('<|reserved_special_token_208|>', 128213),('<|reserved_special_token_209|>', 128214),
    ('<|reserved_special_token_210|>', 128215),('<|reserved_special_token_211|>', 128216),('<|reserved_special_token_212|>', 128217),
    ('<|reserved_special_token_213|>', 128218),('<|reserved_special_token_214|>', 128219),('<|reserved_special_token_215|>', 128220),
    ('<|reserved_special_token_216|>', 128221),('<|reserved_special_token_217|>', 128222),('<|reserved_special_token_218|>', 128223),
    ('<|reserved_special_token_219|>', 128224),('<|reserved_special_token_220|>', 128225),
    ('<|reserved_special_token_221|>', 128226),('<|reserved_special_token_222|>', 128227),
    ('<|reserved_special_token_223|>', 128228),('<|reserved_special_token_224|>', 128229),
    ('<|reserved_special_token_225|>', 128230),('<|reserved_special_token_226|>', 128231),
    ('<|reserved_special_token_227|>', 128232),('<|reserved_special_token_228|>', 128233),
    ('<|reserved_special_token_229|>', 128234),('<|reserved_special_token_230|>', 128235),('<|reserved_special_token_231|>', 128236),('<|reserved_special_token_232|>', 128237),('<|reserved_special_token_233|>', 128238),('<|reserved_special_token_234|>', 128239),('<|reserved_special_token_235|>', 128240),('<|reserved_special_token_236|>', 128241),('<|reserved_special_token_237|>', 128242),('<|reserved_special_token_238|>', 128243),('<|reserved_special_token_239|>', 128244),
    ('<|reserved_special_token_240|>', 128245),('<|reserved_special_token_241|>', 128246),('<|reserved_special_token_242|>', 128247),('<|reserved_special_token_243|>', 128248), ('<|reserved_special_token_244|>', 128249),('<|reserved_special_token_245|>', 128250),('<|reserved_special_token_246|>', 128251),('<|reserved_special_token_247|>', 128252),('<|reserved_special_token_248|>', 128253),('<|reserved_special_token_249|>', 128254),('<|reserved_special_token_250|>', 128255),
]
Fuyu
[
    ('sâ–flu', 262109), ('allâ–our', 262110), ('ogâ–i', 262111), ('tâ–tri', 262112), ('ankâ–you', 262113),('niaâ–i', 262114), ('leâ–tr', 262115), ('sâ–doingâ–the', 262116),
    ('makesâ–the', 262117), ('createT', 262118), ('refectur', 262119), ('aditional', 262120), ('staken', 262121), ('â–aâ–Mar', 262122), ('â–theâ–Dis', 262123), ('â–otherâ–dev', 262124), 
    ('servato', 262125), ('othersâ–who', 262126), ('deâ–pos', 262127), ('esirable', 262128), ('signâ–an', 262129), ('ageâ–rating', 262130), ('lementation', 262131), ('wonderingâ–what', 262132),
    ('stâ–atâ–the', 262133), ('whichâ–isâ–an', 262134), ('inâ–lightâ–ofâ–the', 262135), ('pliedâ–by', 262136),('seâ–det', 262137), ('avies', 262138),
    ('ofâ–pro', 262139), ('â–theâ–pheno', 262140), ('Thisâ–studyâ–was', 262141), ('ionâ–temperature', 262142), ('causeâ–everyone', 262143)
]
'''

def map_control_token(num:int, place:int, tokenizer_type:str = "llama-2",not_text=False) -> str:
    if tokenizer_type == "llama-2":
        special_tokens = [
            (('ì§„', 31536),('à¦œ', 31537),('ì²œ', 31563),('ë…„', 31571),('ì„¸', 31578),('ë¯¼', 31582),('àµ¼', 31585),('á¼¡', 31598),('í˜¸', 31603),('à¨°', 31604),),
            (('ê·¸', 31607),('à¶±', 31609),('à½“', 31614),),
            (('ã‚†', 31621),('ã”', 31622),('í˜„', 31680),),
            (('êµ°', 31699), ('ë¬´', 31716), ('ìœ„', 31724),),
            (('ì•ˆ', 31734), ('ë°•', 31736),),
            (('ìš©', 31737), ('ë‹¨', 31746),),
            (('ë©´', 31747), ('ë‚¨', 31754),),
            (('ê°•', 31774), ('ì”¨', 31781),),
            #(('ê°œ', 31789), ('ë“¤', 31804),),
            (('ì°¨', 31817), ('í•™', 31822), ('ë§Œ', 31826), ('í„°', 31856), ('ì‹', 31895), ('ê³¼', 31906), ('íƒ€', 31925), ('ì¢…', 31930), ('ë‚´', 31940), ('ì¤‘', 31941), ('ë°©', 31945)),
            (('ì›”', 31950), ('íšŒ', 31953), ('ëª¨', 31962), ('ë°”', 31963), ('ìŒ', 31966), ('ì¬', 31973), ('ëª…', 31976), ('í•©', 31980), ('ì—­', 31987), ('ë°±', 31989), ('ì™•', 31996)),
        ]
    elif tokenizer_type == "llava-v1.6":
        special_tokens = [
            (('æœ±', 31947),('Ç', 31948),('á¸¨', 31949),('æ‹…', 31950),('ç°', 31951), ('è®²', 31952), ('ë¡¤', 31953),('ğŸ˜¤', 31955),('áŸ„', 31956),('ì• ', 31957),),
            (('ì˜€', 31958),('ì§ˆ', 31959),('æŒ¯', 31960),),
            (('ç¯', 31961),('Ä‰', 31962),('à·ƒ', 31963),),
            (('é–‰', 31964),('ë¨', 31965),('à²‚', 31966),),
            (('ã’', 31967),('ãµ', 31896),),
            (('ç‹‚', 31969),('è', 31970),),
            (('ä»', 31971),('å¯¦', 31972),),
            (('æ¥½', 31973),('ç¯„', 31974),),
            #(('à°µ', 31976),('åµŒ', 31977),),
            (('æ‘©', 31978),('è¢', 31979),('à¦·', 31980),('ä¹', 31981),('ê·œ', 31982),('å²—', 31983),('ç³Š', 31984),('à°•', 31985),('é›²', 31986),('ì‹¬', 31987),('à¤ˆ', 31988),),
            (('à½ ', 31989),('á¼¡', 31990),('ä¸', 31991),('Ä¦', 31992),('ä¼', 31993),('ì»¨', 31885),('á€¡', 31995),('åŸ·', 31996),('ë²¨', 31997),('ã‚¼', 31998),('æ¢¦', 31999),),
        ]
    elif tokenizer_type == "llama-3":
        special_tokens = [
            (('<|reserved_special_token_200|>', 128205),('<|reserved_special_token_201|>', 128206),('<|reserved_special_token_202|>', 128207),('<|reserved_special_token_203|>', 128208),('<|reserved_special_token_204|>', 128209),('<|reserved_special_token_205|>', 128210),('<|reserved_special_token_206|>', 128211),('<|reserved_special_token_207|>', 128212),('<|reserved_special_token_208|>', 128213),('<|reserved_special_token_209|>', 128214),),
            (('<|reserved_special_token_210|>', 128215),('<|reserved_special_token_211|>', 128216),('<|reserved_special_token_212|>', 128217),),
            (('<|reserved_special_token_213|>', 128218),('<|reserved_special_token_214|>', 128219),('<|reserved_special_token_215|>', 128220),),
            (('<|reserved_special_token_216|>', 128221),('<|reserved_special_token_217|>', 128222),('<|reserved_special_token_218|>', 128223),),
            (('<|reserved_special_token_219|>', 128224),('<|reserved_special_token_220|>', 128225),),
            (('<|reserved_special_token_221|>', 128226),('<|reserved_special_token_222|>', 128227),),
            (('<|reserved_special_token_223|>', 128228),('<|reserved_special_token_224|>', 128229),),
            (('<|reserved_special_token_225|>', 128230),('<|reserved_special_token_226|>', 128231),),
            #(('<|reserved_special_token_227|>', 128232),('<|reserved_special_token_228|>', 128233),),
            (('<|reserved_special_token_229|>', 128234),('<|reserved_special_token_230|>', 128235),('<|reserved_special_token_231|>', 128236),('<|reserved_special_token_232|>', 128237),('<|reserved_special_token_233|>', 128238),('<|reserved_special_token_234|>', 128239),('<|reserved_special_token_235|>', 128240),('<|reserved_special_token_236|>', 128241),('<|reserved_special_token_237|>', 128242),('<|reserved_special_token_238|>', 128243),('<|reserved_special_token_239|>', 128244),),
            (('<|reserved_special_token_240|>', 128245),('<|reserved_special_token_241|>', 128246),('<|reserved_special_token_242|>', 128247),('<|reserved_special_token_243|>', 128248), ('<|reserved_special_token_244|>', 128249),('<|reserved_special_token_245|>', 128250),('<|reserved_special_token_246|>', 128251),('<|reserved_special_token_247|>', 128252),('<|reserved_special_token_248|>', 128253),('<|reserved_special_token_249|>', 128254),('<|reserved_special_token_250|>', 128255),),
        ]
    else:
        raise ValueError(f"The tokenizer type {tokenizer_type} is not supported in control tokens.")
    return special_tokens[place][num][not_text]

def prepare_for_remap_control_token(tokenizer_type:str = "llama-2",bases:list = [10,3,3,3,2,2,2,2,2,11,11]):
    
    tokens = {}
    for i,base in enumerate(bases):
        for j in range(base):
            token = map_control_token(j,i,tokenizer_type)
            tokens[token]=(i,j)
    return tokens

def remap_control_token(token:str, tokenizer_type:str = "llama-2")->tuple:
    """ç”±tokenæ˜ å°„åˆ°actionï¼Œæ³¨æ„ï¼Œè™½ç„¶æŠŠcameraä»tokenä¸­å»æ‰ï¼Œä½†æ˜¯è¿˜éœ€è¦å®ƒ """
    re_tokens = {}
    if tokenizer_type == "llama-2":
        re_tokens = {
            'ì§„': (0, 0),'à¦œ': (0, 1),'ì²œ': (0, 2),'ë…„': (0, 3),'ì„¸': (0, 4),'ë¯¼': (0, 5),'àµ¼': (0, 6),'á¼¡': (0, 7),'í˜¸': (0, 8),'à¨°': (0, 9),
            'ê·¸': (1, 0),'à¶±': (1, 1),'à½“': (1, 2),
            'ã‚†': (2, 0),'ã”': (2, 1),'í˜„': (2, 2),
            'êµ°': (3, 0),'ë¬´': (3, 1),'ìœ„': (3, 2),
            'ì•ˆ': (4, 0),'ë°•': (4, 1),
            'ìš©': (5, 0),'ë‹¨': (5, 1),
            'ë©´': (6, 0),'ë‚¨': (6, 1),
            'ê°•': (7, 0),'ì”¨': (7, 1),
            'ê°œ': (8, 0),'ë“¤': (8, 1),
            'ì°¨': (9, 0),'í•™': (9, 1),'ë§Œ': (9, 2),'í„°': (9, 3),'ì‹': (9, 4),'ê³¼': (9, 5),'íƒ€': (9, 6),'ì¢…': (9, 7),'ë‚´': (9, 8),'ì¤‘': (9, 9),'ë°©': (9, 10),
            'ì›”': (10, 0),'íšŒ': (10, 1),'ëª¨': (10, 2),'ë°”': (10, 3),'ìŒ': (10, 4),'ì¬': (10, 5),'ëª…': (10, 6),'í•©': (10, 7),'ì—­': (10, 8),'ë°±': (10, 9),'ì™•': (10, 10)
        }
    elif tokenizer_type=="llava-v1.6":
        re_tokens = {
            'æœ±': (0, 0),'Ç': (0, 1),'á¸¨': (0, 2),'æ‹…': (0, 3),'ç°': (0, 4),'è®²': (0, 5),'ë¡¤': (0, 6),'ğŸ˜¤': (0, 7),'áŸ„': (0, 8),'ì• ': (0, 9),
            'ì˜€': (1, 0),'ì§ˆ': (1, 1),'æŒ¯': (1, 2),
            'ç¯': (2, 0),'Ä‰': (2, 1),'à·ƒ': (2, 2),
            'é–‰': (3, 0),'ë¨': (3, 1),'à²‚': (3, 2),
            'ã’': (4, 0),'ãµ': (4, 1),
            'ç‹‚': (5, 0),'è': (5, 1),
            'ä»': (6, 0),'å¯¦': (6, 1),
            'æ¥½': (7, 0),'ç¯„': (7, 1),
            'à°µ': (8, 0),'åµŒ': (8, 1),
            'æ‘©': (9, 0),'è¢': (9, 1),'à¦·': (9, 2),'ä¹': (9, 3),'ê·œ': (9, 4),'å²—': (9, 5),'ç³Š': (9, 6),'à°•': (9, 7),'é›²': (9, 8),'ì‹¬': (9, 9),'à¤ˆ': (9, 10),
            'à½ ': (10, 0),'á¼¡': (10, 1),'ä¸': (10, 2),'Ä¦': (10, 3),'ä¼': (10, 4),'ì»¨': (10, 5),'á€¡': (10, 6),'åŸ·': (10, 7),'ë²¨': (10, 8),'ã‚¼': (10, 9),'æ¢¦': (10, 10)
        }
    elif tokenizer_type=="llama-3":
        re_tokens = {
            '<|reserved_special_token_200|>': (0, 0),'<|reserved_special_token_201|>': (0, 1),'<|reserved_special_token_202|>': (0, 2),'<|reserved_special_token_203|>': (0, 3),'<|reserved_special_token_204|>': (0, 4),'<|reserved_special_token_205|>': (0, 5),'<|reserved_special_token_206|>': (0, 6),'<|reserved_special_token_207|>': (0, 7),'<|reserved_special_token_208|>': (0, 8),'<|reserved_special_token_209|>': (0, 9),
            '<|reserved_special_token_210|>': (1, 0),'<|reserved_special_token_211|>': (1, 1),'<|reserved_special_token_212|>': (1, 2),
            '<|reserved_special_token_213|>': (2, 0),'<|reserved_special_token_214|>': (2, 1),'<|reserved_special_token_215|>': (2, 2),
            '<|reserved_special_token_216|>': (3, 0),'<|reserved_special_token_217|>': (3, 1),'<|reserved_special_token_218|>': (3, 2),
            '<|reserved_special_token_219|>': (4, 0),'<|reserved_special_token_220|>': (4, 1),
            '<|reserved_special_token_221|>': (5, 0),'<|reserved_special_token_222|>': (5, 1),
            '<|reserved_special_token_223|>': (6, 0),'<|reserved_special_token_224|>': (6, 1),
            '<|reserved_special_token_225|>': (7, 0),'<|reserved_special_token_226|>': (7, 1),
            '<|reserved_special_token_227|>': (8, 0),'<|reserved_special_token_228|>': (8, 1),
            '<|reserved_special_token_229|>': (9, 0),'<|reserved_special_token_230|>': (9, 1),'<|reserved_special_token_231|>': (9, 2),'<|reserved_special_token_232|>': (9, 3),'<|reserved_special_token_233|>': (9, 4),'<|reserved_special_token_234|>': (9, 5),'<|reserved_special_token_235|>': (9, 6),'<|reserved_special_token_236|>': (9, 7),'<|reserved_special_token_237|>': (9, 8),'<|reserved_special_token_238|>': (9, 9),'<|reserved_special_token_239|>': (9, 10),
            '<|reserved_special_token_240|>': (10, 0),'<|reserved_special_token_241|>': (10, 1),'<|reserved_special_token_242|>': (10, 2),'<|reserved_special_token_243|>': (10, 3),'<|reserved_special_token_244|>': (10, 4),'<|reserved_special_token_245|>': (10, 5),'<|reserved_special_token_246|>': (10, 6),'<|reserved_special_token_247|>': (10, 7),'<|reserved_special_token_248|>': (10, 8),'<|reserved_special_token_249|>': (10, 9),'<|reserved_special_token_250|>': (10, 10)
        }
    else:
        raise ValueError(f"The tokenizer type {tokenizer_type} is not supported in control tokens.")
    return re_tokens.get(token,(-1,-1))


def tag_token(place, tokenizer_type:str = "llama-2"):
    """å¼•å…¥å¤´æ ‡è®°å’Œå°¾æ ‡è®° """
    assert place in {0,1}
    if tokenizer_type == "llama-2":
        special_tokens = [('ìœ ', 31533),('ìš”', 31527)]
    elif tokenizer_type == "llava-v1.6":
        special_tokens = [('à²®', 31941),('áŠ ', 31942)]
    elif tokenizer_type=="llama-3":
        special_tokens = [('<|reserved_special_token_199|>', 128204),('<|reserved_special_token_198|>', 128203)]
    else:
        raise ValueError(f"The tokenizer type {tokenizer_type} is not supported in control tokens.")
    return special_tokens[place][0]



def token_2_action(tokens:str, tokenizer_type:str = 'llama-2',bases:list = [10,3,3,3,2,2,2,2,2,11,11]) -> tuple:
    """å°†ä¸€ä¸ªè¾“å…¥åºåˆ—è½¬æ¢å› """
    pattern = f'{tag_token(0,tokenizer_type)}.*{tag_token(1,tokenizer_type)}'
    match = re.search(pattern, tokens)
    actions = [0]*len(bases) #åˆå§‹åŒ–
    camera_null = [bases[-1]//2,bases[-2]//2]
    if not match:
        return actions

    control_tokens = match.group()[1:-1]
    for token in control_tokens:
        place,num = remap_control_token(token,tokenizer_type)
        if place!=-1:
            actions[place]=num
    # å¦‚æœç§»åŠ¨äº†è§†é‡ï¼Œcamera buttonå˜ä¸º1
    if actions[-2:] != [bases[-1]//2,bases[-2]//2]:
        actions[-3] = 1
    outputs = custom_seq_2_decimal(actions)
    return outputs
        


def action_2_token(inputs:tuple, tokenizer_type:str = 'llama-2'):
    '''
    Params: 
    inputs:tuple:ä¸¤ä¸ªåè¿›åˆ¶æ•°å­—
    * output: str, è¿”å›ä¸€ä¸ªæ§åˆ¶token
    Function: ç”Ÿæˆä¸€ä¸ªæ§åˆ¶token
    Examples:
    1. generate_control_token(15359) -> tuple(7,7,7,5,4) -> 'ë‹¨í•™ì¤‘ìŒë°±'
    2. generate_control_token(1) -> 'tuple(0,0,0,0,1) -> 'í˜„ë©´ë§Œë°©ì¬'
    '''
    # ç”Ÿæˆæ§åˆ¶token
    assert len(inputs)==2
    null_action = (0,60)
    custom_seq = decimal_2_custom_seq(inputs)
    zero_include_token_list = [map_control_token(num, i, tokenizer_type) for i, num in enumerate(custom_seq)]
    control_token = ''.join((s for x,s in zip(custom_seq[:-2],zero_include_token_list[:-2]) if x != 0 ))
    control_token = control_token + "".join((s for s in zero_include_token_list[-2:]))  #cameraå¿…é¡»ä¿å­˜
    tag_control_token = tag_token(0,tokenizer_type) + control_token + tag_token(1,tokenizer_type)
    return tag_control_token,inputs==null_action

def decimal_2_custom_seq(inputs:tuple, bases:list = [10,3,3,3,2,2,2,2,11,11]) -> tuple:
    '''
    Params:
    * output: set, è¿”å›ä¸€ä¸ªå…ƒç»„, å…ƒç»„ä¸­çš„æ¯ä¸ªå…ƒç´ è¡¨ç¤ºä¸€ä¸ªä½çš„å€¼
    * inputs: tuple, ä¸¤ä¸ªåè¿›åˆ¶æ•´æ•°
    * bases: list, æ¯ä½çš„åŸºæ•°     

    Function: å°†ä¸€ä¸ªåè¿›åˆ¶æ•´æ•°è½¬æ¢ä¸ºå…·æœ‰ä¸åŒåŸºæ•°çš„æ•°å­—ç³»ç»Ÿ(æ¯ä½çš„åŸºæ•°åˆ†åˆ«ä¸º [8, 8, 8, 6, 5]), éœ€è¦ç¼–å†™ä¸€ä¸ªPythonå‡½æ•°æ¥æ‰§è¡Œé€†å‘è®¡ç®—ã€‚è¿™ä¸ªè½¬æ¢æ¶‰åŠå°†åè¿›åˆ¶æ•°é€ä½é™¤ä»¥å¯¹åº”çš„åŸºæ•°å¹¶å–ä½™æ•°, ç„¶åå†ç»§ç»­å¤„ç†å•†ã€‚
    Examples: 
    1. decimal_to_custom_base(1) -> (0, 0, 0, 0, 1)
    2. decimal_to_custom_base(15359) -> (7, 7, 7, 5, 4)
    '''
    decimals = list(inputs)
    decimals[0] = decimals[0]//2 #cameraé”®åœ¨è¿™é‡Œæ²¡æœ‰æ„ä¹‰
    # ç”¨äºå­˜å‚¨è½¬æ¢ç»“æœçš„åˆ—è¡¨
    result = [0] * len(bases)
    # ä»æœ€ä½ä½åˆ°æœ€é«˜ä½é€ä½è®¡ç®—
    for i in range(len(bases)-3, -1, -1):
        # æ±‚å½“å‰ä½çš„å€¼
        result[i] = decimals[0] % bases[i]
        # æ›´æ–°åè¿›åˆ¶æ•°ä¸ºä¸‹ä¸€ä½çš„å¤„ç†
        decimals[0] //= bases[i]
    # ç¡®ä¿è½¬æ¢è¿‡ç¨‹ä¸­åè¿›åˆ¶æ•°è¢«å®Œå…¨è½¬æ¢
    result[-1] = decimals[1] % bases[-1]
    decimals[1] //= bases[-1]
    result[-2] = decimals[1] % bases[-2]
    decimals[1] //= bases[-2]

    if decimals != [0,0]:
        raise ValueError("The decimal number is too large for the custom base system.")
    return tuple(result)


def custom_seq_2_decimal(number_tuple:tuple, bases:list = [10,3,3,3,2,2,2,2,2,11,11]) -> tuple:
    '''
    å‡å¦‚basesä¸º[10,3,3,3,2,2,2,2,2,11,11]
    Function: å°†ä¸€ä¸ªå…·æœ‰ä¸åŒåŸºæ•°çš„æ•°å­—ç³»ç»Ÿ(æ¯ä½çš„åŸºæ•°åˆ†åˆ«ä¸º [8, 8, 8, 6, 5])è½¬æ¢ä¸ºåè¿›åˆ¶æ•´æ•°, éœ€è¦ç¼–å†™ä¸€ä¸ªPythonå‡½æ•°æ¥æ‰§è¡Œé€†å‘è®¡ç®—ã€‚è¿™ä¸ªè½¬æ¢æ¶‰åŠå°†æ¯ä½çš„å€¼ä¹˜ä»¥å¯¹åº”çš„åŸºæ•°çš„å¹‚, ç„¶åå†æ±‚å’Œã€‚
    Examples:
    1. custom_base_to_decimal((0, 0, 0, 0, 1)) -> 1
    2. custom_base_to_decimal((7, 7, 7, 5, 4)) -> 15359
    :output: int, åè¿›åˆ¶æ•´æ•°
    :number_tuple: tuple, æ¯ä½çš„å€¼
    :bases: list, æ¯ä½çš„åŸºæ•°
    '''
    # ç¡®ä¿è¾“å…¥çš„é•¿åº¦ä¸åŸºæ•°åŒ¹é…
    if len(number_tuple) != len(bases):
        raise ValueError("The input number does not match the expected number of digits.")
    # åˆå§‹åŒ–åè¿›åˆ¶ç»“æœ
    decimal_results = [0,0]
    # è®¡ç®—åè¿›åˆ¶å€¼
    mid = len(number_tuple)-2
    for i, digit in enumerate(number_tuple):
        if digit >= bases[i]:
            raise ValueError(f"Digit at position {i} exceeds the base limit of {bases[i]-1}.")
        if i < mid:
            decimal_results[0] = decimal_results[0] * bases[i] + digit
        else:
            decimal_results[1] = decimal_results[1] * bases[i] + digit
    return tuple(decimal_results)


class ActionMap:
    def __init__(self,tokenizer_type="llama-2",bases=[10,3,3,3,2,2,2,2,2,11,11]):
        self.tokenizer_type = tokenizer_type
        self.bases = bases
        self.action_transformer = ActionTransformer()
        self.action_mapper = CameraHierarchicalMapping(n_camera_bins=11)
    
    def map(self,token):
        action = token_2_action(token,tokenizer_type=self.tokenizer_type,bases=self.bases)
        action_dict = {
            "buttons":np.array([action[0]]),
            "camera":np.array([action[1]]),
        }
        action_dict = OrderedDict({key: value[0] for key, value in action_dict.items()})
        #factored_action = self.action_mapper.to_factored(action_dict)
        #envir_action = self.action_transformer.policy2env(factored_action)
        return action_dict
    
if __name__ == "__main__":
    action_map = ActionMap("llama-3")
    print(action_map.map("<|reserved_special_token_199|><|reserved_special_token_234|><|reserved_special_token_245|><|reserved_special_token_198|>"))